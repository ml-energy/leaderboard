{
  "configurations": [
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.997300215982722,
      "avg_output_len": 7026.242424242424,
      "avg_power_watts": 319.7883095908309,
      "data_parallel": 1,
      "energy_per_request_joules": 9599.698100812033,
      "energy_per_token_joules": 1.3662634337367148,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 33.62808651581872,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 234.06050523961807,
      "p90_itl_ms": 51.68297459313183,
      "p95_itl_ms": 57.232036642381004,
      "p99_itl_ms": 68.6399259546306,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.99621081639683,
      "avg_output_len": 7304.171717171717,
      "avg_power_watts": 360.8975244155932,
      "data_parallel": 1,
      "energy_per_request_joules": 5701.949657261465,
      "energy_per_token_joules": 0.7806428816365977,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 34.790393998264335,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 462.3080961924367,
      "p90_itl_ms": 50.95546791271772,
      "p95_itl_ms": 56.04782736045309,
      "p99_itl_ms": 66.4822291364544,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.990549828178693,
      "avg_output_len": 6907.257575757576,
      "avg_power_watts": 422.34231413670085,
      "data_parallel": 1,
      "energy_per_request_joules": 2887.6262752471016,
      "energy_per_token_joules": 0.4180568400086617,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 30.597431992646307,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1010.2509365184657,
      "p90_itl_ms": 46.504846977768466,
      "p95_itl_ms": 51.50576701271348,
      "p99_itl_ms": 61.93683718447574,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 63.985365853658536,
      "avg_output_len": 6989.464646464647,
      "avg_power_watts": 507.16354556998795,
      "data_parallel": 1,
      "energy_per_request_joules": 2035.1590661341565,
      "energy_per_token_joules": 0.2911752428941126,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 36.26766199886333,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1741.781136778928,
      "p90_itl_ms": 53.20127041486558,
      "p95_itl_ms": 58.62280293949878,
      "p99_itl_ms": 70.55523046030432,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 127.98625429553265,
      "avg_output_len": 7327.010101010101,
      "avg_power_watts": 511.25871623968027,
      "data_parallel": 1,
      "energy_per_request_joules": 1337.7320344211416,
      "energy_per_token_joules": 0.18257543199465798,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 48.83954499382526,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2800.260202887754,
      "p90_itl_ms": 68.44392910425087,
      "p95_itl_ms": 74.06184781284536,
      "p99_itl_ms": 86.26595737849128,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.998330550918197,
      "avg_output_len": 6930.373737373738,
      "avg_power_watts": 483.6227004443375,
      "data_parallel": 1,
      "energy_per_request_joules": 6057.803040527457,
      "energy_per_token_joules": 0.8740947126500942,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 14.42334707826376,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 553.2840931826285,
      "p90_itl_ms": 15.312102250754833,
      "p95_itl_ms": 15.615710522979498,
      "p99_itl_ms": 18.57272390276194,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.997258396161754,
      "avg_output_len": 7432.025252525253,
      "avg_power_watts": 494.269463945933,
      "data_parallel": 1,
      "energy_per_request_joules": 3975.491083842459,
      "energy_per_token_joules": 0.5349135597314698,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 17.340729013085365,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 924.0174509579821,
      "p90_itl_ms": 18.916429951786995,
      "p95_itl_ms": 19.361893832683563,
      "p99_itl_ms": 21.631541699171073,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.998768472906406,
      "avg_output_len": 6939.30808080808,
      "avg_power_watts": 525.7126515077016,
      "data_parallel": 1,
      "energy_per_request_joules": 2455.5661684249762,
      "energy_per_token_joules": 0.3538632583868544,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 21.54853194952011,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1485.6378531759747,
      "p90_itl_ms": 24.447074532508854,
      "p95_itl_ms": 25.796907395124435,
      "p99_itl_ms": 28.142292946577065,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 14.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 59.014256619144604,
      "avg_output_len": 6710.813131313132,
      "avg_power_watts": 600.0061181329069,
      "data_parallel": 1,
      "energy_per_request_joules": 1876.0888043234238,
      "energy_per_token_joules": 0.279562069098521,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 28.065836057066917,
      "model_id": "Qwen/Qwen3-14B",
      "nickname": "Qwen 3 14B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2146.2357896680815,
      "p90_itl_ms": 29.982031136751175,
      "p95_itl_ms": 31.135702878236774,
      "p99_itl_ms": 34.547596350312254,
      "tensor_parallel": 1,
      "total_params_billions": 14.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999239543726236,
      "avg_output_len": 8928.247474747475,
      "avg_power_watts": 2526.5910915868817,
      "data_parallel": 1,
      "energy_per_request_joules": 53815.86792273631,
      "energy_per_token_joules": 6.0275959055736665,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 19.00674100033939,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 419.17061647257486,
      "p90_itl_ms": 19.77367239887826,
      "p95_itl_ms": 19.96318690071348,
      "p99_itl_ms": 20.342997779371217,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.997944923962187,
      "avg_output_len": 8978.045454545454,
      "avg_power_watts": 2824.1708935463976,
      "data_parallel": 1,
      "energy_per_request_joules": 36995.06988607394,
      "energy_per_token_joules": 4.120615124235518,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 23.20527301344555,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 685.376044206593,
      "p90_itl_ms": 24.29299699724652,
      "p95_itl_ms": 24.595946504268795,
      "p99_itl_ms": 25.258268608304206,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99716914366596,
      "avg_output_len": 8993.79292929293,
      "avg_power_watts": 3023.446090891989,
      "data_parallel": 1,
      "energy_per_request_joules": 24284.199819156256,
      "energy_per_token_joules": 2.70010661909529,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 28.4514160011895,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 1119.7506311454613,
      "p90_itl_ms": 29.4552395993378,
      "p95_itl_ms": 29.753016802715138,
      "p99_itl_ms": 34.008353518438525,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 63.99640287769784,
      "avg_output_len": 9166.0101010101,
      "avg_power_watts": 2952.2148136474398,
      "data_parallel": 1,
      "energy_per_request_joules": 16599.308592006815,
      "energy_per_token_joules": 1.8109633754579388,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 38.86479849461466,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 1630.1902366749478,
      "p90_itl_ms": 40.81023189792177,
      "p95_itl_ms": 41.45626680547139,
      "p99_itl_ms": 45.303142155898975,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 127.99693251533742,
      "avg_output_len": 8996.772727272728,
      "avg_power_watts": 3069.47977720058,
      "data_parallel": 1,
      "energy_per_request_joules": 9562.915772799734,
      "energy_per_token_joules": 1.0629273476933352,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 44.25876399909612,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 2887.760658206486,
      "p90_itl_ms": 46.3336097978754,
      "p95_itl_ms": 46.72891469672322,
      "p99_itl_ms": 51.27377211902056,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 225.61234567901235,
      "avg_output_len": 9149.81313131313,
      "avg_power_watts": 3232.5291155315317,
      "data_parallel": 1,
      "energy_per_request_joules": 6995.096180724975,
      "energy_per_token_joules": 0.7645069992507134,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 50.343298506049905,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 4228.253133980074,
      "p90_itl_ms": 55.78704259096412,
      "p95_itl_ms": 61.40488509699935,
      "p99_itl_ms": 87.17403255635872,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.998900192466318,
      "avg_output_len": 9262.717171717171,
      "avg_power_watts": 3936.581511491244,
      "data_parallel": 1,
      "energy_per_request_joules": 75623.0372761953,
      "energy_per_token_joules": 8.164239053644332,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 16.536694999558676,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 482.173719513277,
      "p90_itl_ms": 17.051538100440666,
      "p95_itl_ms": 17.23856009989504,
      "p99_itl_ms": 17.91685269969094,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.998505976095618,
      "avg_output_len": 9379.176767676769,
      "avg_power_watts": 4490.430254043855,
      "data_parallel": 1,
      "energy_per_request_joules": 49666.36973049346,
      "energy_per_token_joules": 5.29538689383246,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 18.77472799969837,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 847.9890788100605,
      "p90_itl_ms": 19.34954220050713,
      "p95_itl_ms": 19.52078710000933,
      "p99_itl_ms": 20.0685828800124,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.996603260869566,
      "avg_output_len": 9470.949494949495,
      "avg_power_watts": 4422.0779458838015,
      "data_parallel": 1,
      "energy_per_request_joules": 37697.04603686624,
      "energy_per_token_joules": 3.9802816029130628,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 28.752029000315815,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1110.9962527896016,
      "p90_itl_ms": 29.682929000045988,
      "p95_itl_ms": 29.91489424912288,
      "p99_itl_ms": 30.604673221114354,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 63.9958217270195,
      "avg_output_len": 9434.469696969696,
      "avg_power_watts": 4793.243089035774,
      "data_parallel": 1,
      "energy_per_request_joules": 22688.389997643946,
      "energy_per_token_joules": 2.4048399885084524,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 32.18659099911747,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1993.165080396336,
      "p90_itl_ms": 34.790948799127364,
      "p95_itl_ms": 35.317503699843655,
      "p99_itl_ms": 36.864018660962756,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 128.0,
      "avg_output_len": 9106.767676767677,
      "avg_power_watts": 5107.914367077919,
      "data_parallel": 1,
      "energy_per_request_joules": 12582.1711766103,
      "energy_per_token_joules": 1.3816286549956407,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 34.683529500398436,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 3697.024051005975,
      "p90_itl_ms": 37.60676530000637,
      "p95_itl_ms": 38.15989609975077,
      "p99_itl_ms": 40.900465580416494,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 255.9760479041916,
      "avg_output_len": 9251.717171717171,
      "avg_power_watts": 5324.49080996465,
      "data_parallel": 1,
      "energy_per_request_joules": 8932.289025407115,
      "energy_per_token_joules": 0.9654736369063941,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 47.09071849993052,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 5514.900258722318,
      "p90_itl_ms": 64.95665810016362,
      "p95_itl_ms": 68.05556069939483,
      "p99_itl_ms": 71.2804756193327,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 383.97083333333336,
      "avg_output_len": 9302.223905723906,
      "avg_power_watts": 5056.908262090388,
      "data_parallel": 1,
      "energy_per_request_joules": 8018.700718672469,
      "energy_per_token_joules": 0.8620197492492466,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 384,
      "median_itl_ms": 83.80036599919549,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 5866.348498969506,
      "p90_itl_ms": 127.39416920085206,
      "p95_itl_ms": 133.42911469844697,
      "p99_itl_ms": 141.98335292123383,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 511.95979899497485,
      "avg_output_len": 8949.828282828283,
      "avg_power_watts": 5513.329773292434,
      "data_parallel": 1,
      "energy_per_request_joules": 5670.521447551582,
      "energy_per_token_joules": 0.6335899716010652,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 81.84408800116216,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 8701.731435806021,
      "p90_itl_ms": 108.1637857005262,
      "p95_itl_ms": 113.19406614966282,
      "p99_itl_ms": 130.75747443937868,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 695.1068580542264,
      "avg_output_len": 9061.727272727272,
      "avg_power_watts": 3965.734072480003,
      "data_parallel": 1,
      "energy_per_request_joules": 7054.476971073819,
      "energy_per_token_joules": 0.7784914242900913,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 768,
      "median_itl_ms": 124.03286350127019,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 5094.126857076643,
      "p90_itl_ms": 169.87547609896868,
      "p95_itl_ms": 180.5213226496562,
      "p99_itl_ms": 262.3587538905859,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.99954233409611,
      "avg_output_len": 9194.530303030304,
      "avg_power_watts": 2837.254751044061,
      "data_parallel": 1,
      "energy_per_request_joules": 66158.021177755,
      "energy_per_token_joules": 7.195367136475786,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 20.23592299883603,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 394.3168843548014,
      "p90_itl_ms": 20.987010200769873,
      "p95_itl_ms": 21.200230199610814,
      "p99_itl_ms": 21.644799660352877,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.99850355405911,
      "avg_output_len": 9198.712121212122,
      "avg_power_watts": 3181.8895714164837,
      "data_parallel": 1,
      "energy_per_request_joules": 45582.05746574394,
      "energy_per_token_joules": 4.955265135500029,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 24.881263001589105,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 642.1229711042301,
      "p90_itl_ms": 25.91734299858217,
      "p95_itl_ms": 26.224371401258395,
      "p99_itl_ms": 26.813625257709646,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99690785405071,
      "avg_output_len": 8955.040404040405,
      "avg_power_watts": 3445.3851031286467,
      "data_parallel": 1,
      "energy_per_request_joules": 32322.508696085035,
      "energy_per_token_joules": 3.609420754986378,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 33.39150100509869,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "nickname": "Qwen 3 235B A22B Thinking",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 954.5534691041165,
      "p90_itl_ms": 34.738079097587615,
      "p95_itl_ms": 35.09773920013686,
      "p99_itl_ms": 35.91120338824112,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999640890591333,
      "avg_output_len": 9525.025252525253,
      "avg_power_watts": 1789.5539496614217,
      "data_parallel": 1,
      "energy_per_request_joules": 78914.32956907076,
      "energy_per_token_joules": 8.284947018712542,
      "expert_parallel": 2,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 36.865409987512976,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 216.00065101436383,
      "p90_itl_ms": 38.00415920559317,
      "p95_itl_ms": 38.25340419425629,
      "p99_itl_ms": 38.73853085213341,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.998467265163127,
      "avg_output_len": 9380.636363636364,
      "avg_power_watts": 1820.7289620274892,
      "data_parallel": 1,
      "energy_per_request_joules": 44012.5351278287,
      "energy_per_token_joules": 4.691849616774552,
      "expert_parallel": 2,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 41.16085100395139,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 388.06208867349915,
      "p90_itl_ms": 42.12684288795572,
      "p95_itl_ms": 42.44355245900805,
      "p99_itl_ms": 44.02511074236828,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.998218262806237,
      "avg_output_len": 9273.833333333334,
      "avg_power_watts": 1856.0364945924946,
      "data_parallel": 1,
      "energy_per_request_joules": 24466.939244319336,
      "energy_per_token_joules": 2.638276790717898,
      "expert_parallel": 2,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 45.45507000875659,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 703.503325019757,
      "p90_itl_ms": 47.16949100838974,
      "p95_itl_ms": 47.4936029931996,
      "p99_itl_ms": 48.10486939968541,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 63.99279279279279,
      "avg_output_len": 9219.156565656565,
      "avg_power_watts": 1896.0118684277618,
      "data_parallel": 1,
      "energy_per_request_joules": 13801.170231738884,
      "energy_per_token_joules": 1.4970100717403316,
      "expert_parallel": 2,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 50.621745991520584,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1266.532473107262,
      "p90_itl_ms": 54.89455460337922,
      "p95_itl_ms": 55.2146983158309,
      "p99_itl_ms": 56.01393941906281,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999441444796127,
      "avg_output_len": 9201.40909090909,
      "avg_power_watts": 2998.178786599502,
      "data_parallel": 1,
      "energy_per_request_joules": 85185.34771218625,
      "energy_per_token_joules": 9.257858972529394,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 24.582963000284508,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 323.852285446983,
      "p90_itl_ms": 25.035851998836733,
      "p95_itl_ms": 25.361088002682664,
      "p99_itl_ms": 25.820872603799216,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.998572957545488,
      "avg_output_len": 9179.191919191919,
      "avg_power_watts": 3046.8798948509034,
      "data_parallel": 1,
      "energy_per_request_joules": 47960.31535984823,
      "energy_per_token_joules": 5.224895152216228,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 27.297725500829984,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 583.1466098527388,
      "p90_itl_ms": 27.86018500046339,
      "p95_itl_ms": 28.062492801836925,
      "p99_itl_ms": 28.684546849253817,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.996610169491525,
      "avg_output_len": 9203.025252525253,
      "avg_power_watts": 3176.8905823146624,
      "data_parallel": 1,
      "energy_per_request_joules": 27161.36564534776,
      "energy_per_token_joules": 2.951351854423615,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 29.477635995135643,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 1076.418786717348,
      "p90_itl_ms": 30.720707000000402,
      "p95_itl_ms": 31.056380990776233,
      "p99_itl_ms": 31.86296799685806,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 63.99706314243759,
      "avg_output_len": 9158.29797979798,
      "avg_power_watts": 3232.0294371704854,
      "data_parallel": 1,
      "energy_per_request_joules": 15326.558865041003,
      "energy_per_token_joules": 1.6735160724022529,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 32.73992599861231,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 1931.2807868830687,
      "p90_itl_ms": 34.281802191981114,
      "p95_itl_ms": 34.79172960505821,
      "p99_itl_ms": 38.61007971281652,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 127.98625429553265,
      "avg_output_len": 9555.156565656565,
      "avg_power_watts": 3261.0152487439477,
      "data_parallel": 1,
      "energy_per_request_joules": 9334.732047824797,
      "energy_per_token_joules": 0.9769313546756497,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 38.30493599525653,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 3338.0188210118768,
      "p90_itl_ms": 40.12780199991539,
      "p95_itl_ms": 40.40211579267634,
      "p99_itl_ms": 43.71592538576806,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 255.96476964769647,
      "avg_output_len": 9116.376262626263,
      "avg_power_watts": 3316.7269159762914,
      "data_parallel": 1,
      "energy_per_request_joules": 5884.949231700578,
      "energy_per_token_joules": 0.6455360180586963,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 52.09324599127285,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 5137.9424589670425,
      "p90_itl_ms": 77.32075320091099,
      "p95_itl_ms": 80.39097959990613,
      "p99_itl_ms": 83.13109963550234,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 383.96559139784944,
      "avg_output_len": 9329.124579124578,
      "avg_power_watts": 3199.2789946423736,
      "data_parallel": 1,
      "energy_per_request_joules": 5073.685102389289,
      "energy_per_token_joules": 0.5438543626850559,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 384,
      "median_itl_ms": 75.13649750762852,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 5882.602428428186,
      "p90_itl_ms": 112.37821849499596,
      "p95_itl_ms": 116.45511425376753,
      "p99_itl_ms": 137.63553394092014,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 511.97630331753555,
      "avg_output_len": 9122.846801346801,
      "avg_power_watts": 3316.7081131846653,
      "data_parallel": 1,
      "energy_per_request_joules": 3810.7485086481047,
      "energy_per_token_joules": 0.41771484182826857,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 82.01268799894024,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 7940.125130981663,
      "p90_itl_ms": 108.26113479852211,
      "p95_itl_ms": 113.80948220321443,
      "p99_itl_ms": 154.2814498289954,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 896.6826783114992,
      "avg_output_len": 8945.40909090909,
      "avg_power_watts": 2102.6552402763327,
      "data_parallel": 1,
      "energy_per_request_joules": 4006.406229306331,
      "energy_per_token_joules": 0.4478728908416165,
      "expert_parallel": 4,
      "gpu_model": "B200",
      "max_num_seqs": 1024,
      "median_itl_ms": 120.10393649688922,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 4694.758899841305,
      "p90_itl_ms": 226.39649560005637,
      "p95_itl_ms": 243.23586514874478,
      "p99_itl_ms": 320.6806679388682,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.998431987455899,
      "avg_output_len": 9046.818181818182,
      "avg_power_watts": 2051.764438968017,
      "data_parallel": 1,
      "energy_per_request_joules": 81621.46681218882,
      "energy_per_token_joules": 9.022118624670423,
      "expert_parallel": 4,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 35.106019116938114,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 227.41492595293468,
      "p90_itl_ms": 35.68514231592417,
      "p95_itl_ms": 35.84449077025056,
      "p99_itl_ms": 36.17498764768243,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.99846508058327,
      "avg_output_len": 8934.484848484848,
      "avg_power_watts": 2085.004197227219,
      "data_parallel": 1,
      "energy_per_request_joules": 44642.247820487115,
      "energy_per_token_joules": 4.996622477686305,
      "expert_parallel": 4,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 38.23172394186258,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 417.28271578217846,
      "p90_itl_ms": 39.07731380313635,
      "p95_itl_ms": 39.27069362252951,
      "p99_itl_ms": 39.93051193654537,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.994897959183675,
      "avg_output_len": 9024.18686868687,
      "avg_power_watts": 2122.508904401888,
      "data_parallel": 1,
      "energy_per_request_joules": 26093.681055706787,
      "energy_per_token_joules": 2.8915271187756044,
      "expert_parallel": 4,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 43.307825922966,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 4,
      "output_throughput_tokens_per_sec": 734.0442669964127,
      "p90_itl_ms": 44.745996594429016,
      "p95_itl_ms": 45.16365844756365,
      "p99_itl_ms": 46.947318501770496,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999106504646176,
      "avg_output_len": 9288.722222222223,
      "avg_power_watts": 3626.973377804078,
      "data_parallel": 1,
      "energy_per_request_joules": 107647.70124582382,
      "energy_per_token_joules": 11.589075296954064,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 25.474630296230316,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 312.9648643112492,
      "p90_itl_ms": 25.978809222579002,
      "p95_itl_ms": 26.115630939602852,
      "p99_itl_ms": 26.377523317933083,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 15.999335327351279,
      "avg_output_len": 9550.550505050505,
      "avg_power_watts": 3695.911597906651,
      "data_parallel": 1,
      "energy_per_request_joules": 61469.32106681325,
      "energy_per_token_joules": 6.436207110187748,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 27.76678465306759,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 574.2375182514658,
      "p90_itl_ms": 28.44787947833538,
      "p95_itl_ms": 28.704164549708366,
      "p99_itl_ms": 29.101931862533092,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 31.998797354179196,
      "avg_output_len": 9531.444444444445,
      "avg_power_watts": 3799.1154640536115,
      "data_parallel": 1,
      "energy_per_request_joules": 36646.28786232262,
      "energy_per_token_joules": 3.8447779951844017,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 32.201044261455536,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 988.1234934271932,
      "p90_itl_ms": 33.301991969347,
      "p95_itl_ms": 33.67263386026025,
      "p99_itl_ms": 34.3761639483273,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 63.99409681227863,
      "avg_output_len": 9019.984848484848,
      "avg_power_watts": 3778.026163898068,
      "data_parallel": 1,
      "energy_per_request_joules": 20843.423672576475,
      "energy_per_token_joules": 2.310804732236074,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 39.5278912037611,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1634.9395996961725,
      "p90_itl_ms": 41.83093644678593,
      "p95_itl_ms": 42.2195203602314,
      "p99_itl_ms": 43.016559258103364,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 95.98238747553816,
      "avg_output_len": 9231.777777777777,
      "avg_power_watts": 3860.946487305547,
      "data_parallel": 1,
      "energy_per_request_joules": 15943.04854607975,
      "energy_per_token_joules": 1.7269749044931488,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 43.42041350901127,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 2235.6702910160116,
      "p90_itl_ms": 46.97277583181858,
      "p95_itl_ms": 47.311057057231665,
      "p99_itl_ms": 48.420037776231766,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 22.0,
      "architecture": "MoE",
      "avg_batch_size": 127.97604790419162,
      "avg_output_len": 9296.89898989899,
      "avg_power_watts": 3918.0969655339563,
      "data_parallel": 1,
      "energy_per_request_joules": 12872.757667985596,
      "energy_per_token_joules": 1.3846291846315368,
      "expert_parallel": 8,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 45.91882973909378,
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
      "nickname": "Qwen 3 235B A22B Thinking FP8",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 2829.7084945357406,
      "p90_itl_ms": 49.58171360194683,
      "p95_itl_ms": 50.07552541792393,
      "p99_itl_ms": 50.85372041910886,
      "tensor_parallel": 1,
      "total_params_billions": 235.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 7.994999242309441,
      "avg_output_len": 7971.888888888889,
      "avg_power_watts": 304.76387398824545,
      "data_parallel": 1,
      "energy_per_request_joules": 10522.292999092948,
      "energy_per_token_joules": 1.3199246936016353,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 35.00107250874862,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 230.89489534182914,
      "p90_itl_ms": 49.47978199925274,
      "p95_itl_ms": 54.143508736160584,
      "p99_itl_ms": 64.42524934682295,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 15.994067237969677,
      "avg_output_len": 8106.378787878788,
      "avg_power_watts": 389.82085814005154,
      "data_parallel": 1,
      "energy_per_request_joules": 6413.084773811704,
      "energy_per_token_joules": 0.7911158535301838,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 32.46195850078948,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 492.7481308844465,
      "p90_itl_ms": 47.92680539540015,
      "p95_itl_ms": 52.92729524662718,
      "p99_itl_ms": 64.06189499393804,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 31.989911727616647,
      "avg_output_len": 8004.954545454545,
      "avg_power_watts": 437.99892250325576,
      "data_parallel": 1,
      "energy_per_request_joules": 3950.743376762825,
      "energy_per_token_joules": 0.49353726549342825,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 35.12723097810522,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 887.4687954218687,
      "p90_itl_ms": 51.04164220392704,
      "p95_itl_ms": 56.551950084394775,
      "p99_itl_ms": 69.769805209944,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 63.98337595907928,
      "avg_output_len": 8298.035353535353,
      "avg_power_watts": 470.6360547173012,
      "data_parallel": 1,
      "energy_per_request_joules": 2528.322119929481,
      "energy_per_token_joules": 0.3046892441657647,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 40.632746007759124,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1544.6428245470127,
      "p90_itl_ms": 55.97567781805993,
      "p95_itl_ms": 61.806749395327614,
      "p99_itl_ms": 78.25580780045065,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 127.97109826589596,
      "avg_output_len": 8436.267676767677,
      "avg_power_watts": 477.13867767070093,
      "data_parallel": 1,
      "energy_per_request_joules": 1631.7971343424108,
      "energy_per_token_joules": 0.1934264294192746,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 56.67889100732282,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2466.7708497914036,
      "p90_itl_ms": 75.91897319653073,
      "p95_itl_ms": 82.61979750532187,
      "p99_itl_ms": 98.56466554396317,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 241.4622222222222,
      "avg_output_len": 8262.189393939394,
      "avg_power_watts": 541.0238473628527,
      "data_parallel": 1,
      "energy_per_request_joules": 1205.2448476546642,
      "energy_per_token_joules": 0.14587475427987087,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 70.95763701363467,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3708.8243955143926,
      "p90_itl_ms": 100.13763199094683,
      "p95_itl_ms": 108.37734199594706,
      "p99_itl_ms": 130.12055170838727,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 459.52244897959184,
      "avg_output_len": 8142.621212121212,
      "avg_power_watts": 544.5395244066468,
      "data_parallel": 1,
      "energy_per_request_joules": 713.6920377342874,
      "energy_per_token_joules": 0.08764893013466918,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 86.69960199040361,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 6212.734411817497,
      "p90_itl_ms": 110.86475100019015,
      "p95_itl_ms": 120.47574820753644,
      "p99_itl_ms": 168.23538968281358,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 7.997072354663321,
      "avg_output_len": 8162.186868686868,
      "avg_power_watts": 519.22159254518,
      "data_parallel": 1,
      "energy_per_request_joules": 6539.15450593015,
      "energy_per_token_joules": 0.8011522660693712,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 12.309283018112183,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 648.0935204647114,
      "p90_itl_ms": 13.100481033325195,
      "p95_itl_ms": 13.35893217474222,
      "p99_itl_ms": 16.339488066732873,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 15.995015576323988,
      "avg_output_len": 8242.959595959595,
      "avg_power_watts": 564.8065033526215,
      "data_parallel": 1,
      "energy_per_request_joules": 4875.6130586143145,
      "energy_per_token_joules": 0.5914881665808681,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 16.748839057981968,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 954.8906221024141,
      "p90_itl_ms": 18.053930439054966,
      "p95_itl_ms": 18.540182430297136,
      "p99_itl_ms": 21.3086342997849,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999461206896552,
      "avg_output_len": 8309.136363636364,
      "avg_power_watts": 710.2988788848096,
      "data_parallel": 1,
      "energy_per_request_joules": 6945.579129713426,
      "energy_per_token_joules": 0.8358966354324942,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 9.367374703288078,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 849.7448712869862,
      "p90_itl_ms": 9.84083116054535,
      "p95_itl_ms": 9.986687451601028,
      "p99_itl_ms": 10.294333472847937,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 15.999084249084248,
      "avg_output_len": 7950.151515151515,
      "avg_power_watts": 787.4286318346727,
      "data_parallel": 1,
      "energy_per_request_joules": 4713.210568061525,
      "energy_per_token_joules": 0.5928453764785513,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 11.967048048973083,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1328.219234013308,
      "p90_itl_ms": 12.794099375605585,
      "p95_itl_ms": 13.057692814618349,
      "p99_itl_ms": 13.601342476904392,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99586206896552,
      "avg_output_len": 8110.404040404041,
      "avg_power_watts": 843.5222415723906,
      "data_parallel": 1,
      "energy_per_request_joules": 3480.96381392695,
      "energy_per_token_joules": 0.4291973367276949,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 16.395246610045433,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1965.3482661463136,
      "p90_itl_ms": 17.110166512429714,
      "p95_itl_ms": 17.271547671407458,
      "p99_itl_ms": 17.595617100596428,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 63.997518610421835,
      "avg_output_len": 8116.545454545455,
      "avg_power_watts": 910.0537943622338,
      "data_parallel": 1,
      "energy_per_request_joules": 2460.302410131322,
      "energy_per_token_joules": 0.30312186679783765,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 21.53947949409485,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 3002.2703540856055,
      "p90_itl_ms": 22.987710312008858,
      "p95_itl_ms": 23.289293609559536,
      "p99_itl_ms": 23.904349207878113,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 95.99350649350649,
      "avg_output_len": 8283.176767676769,
      "avg_power_watts": 915.6778884394571,
      "data_parallel": 1,
      "energy_per_request_joules": 2132.7951653246278,
      "energy_per_token_joules": 0.2574851684497885,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 27.714597061276436,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 3556.2354676674167,
      "p90_itl_ms": 30.28995543718338,
      "p95_itl_ms": 30.615631490945816,
      "p99_itl_ms": 31.410065293312076,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 3.0,
      "architecture": "MoE",
      "avg_batch_size": 127.98863636363636,
      "avg_output_len": 8057.949494949495,
      "avg_power_watts": 926.3172382702305,
      "data_parallel": 1,
      "energy_per_request_joules": 1600.3505155531907,
      "energy_per_token_joules": 0.19860518070462557,
      "expert_parallel": 2,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 28.594817966222763,
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "nickname": "Qwen 3 30B A3B Thinking",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 4664.114173576824,
      "p90_itl_ms": 32.3397321626544,
      "p95_itl_ms": 32.74561371654272,
      "p99_itl_ms": 33.72602770105004,
      "tensor_parallel": 1,
      "total_params_billions": 30.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.997000130429112,
      "avg_output_len": 7334.449494949495,
      "avg_power_watts": 423.00286890341283,
      "data_parallel": 1,
      "energy_per_request_joules": 17258.600354141854,
      "energy_per_token_joules": 2.353087353866999,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 44.14068500045687,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 179.76505130940487,
      "p90_itl_ms": 60.802932805381715,
      "p95_itl_ms": 66.05900719296187,
      "p99_itl_ms": 76.88396901357919,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.992758620689655,
      "avg_output_len": 7041.409090909091,
      "avg_power_watts": 535.4159160187783,
      "data_parallel": 1,
      "energy_per_request_joules": 8475.344485106918,
      "energy_per_token_joules": 1.2036432446524274,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 36.031765514053404,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 444.82941137046697,
      "p90_itl_ms": 51.31638528255281,
      "p95_itl_ms": 56.57351335103158,
      "p99_itl_ms": 67.58948457922088,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.99245283018868,
      "avg_output_len": 6857.540404040404,
      "avg_power_watts": 598.9941425870634,
      "data_parallel": 1,
      "energy_per_request_joules": 4777.615511481555,
      "energy_per_token_joules": 0.6966952041094245,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 36.58818898838945,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 859.7649862578702,
      "p90_itl_ms": 53.038106992607936,
      "p95_itl_ms": 58.64172961446455,
      "p99_itl_ms": 71.60381401889029,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 63.98082595870206,
      "avg_output_len": 7087.555555555556,
      "avg_power_watts": 707.0546636922346,
      "data_parallel": 1,
      "energy_per_request_joules": 3311.581596317252,
      "energy_per_token_joules": 0.4672388908079148,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 41.3123504986288,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1513.261583319077,
      "p90_itl_ms": 61.044121911982074,
      "p95_itl_ms": 67.52462244767236,
      "p99_itl_ms": 82.44491031626241,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 110.90272373540856,
      "avg_output_len": 7034.626262626262,
      "avg_power_watts": 773.7225635974955,
      "data_parallel": 1,
      "energy_per_request_joules": 2192.2789932673077,
      "energy_per_token_joules": 0.31164114644078567,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 44.815598492277786,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2482.7355836483202,
      "p90_itl_ms": 66.26662090129686,
      "p95_itl_ms": 73.40900139533916,
      "p99_itl_ms": 93.98568142205445,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.998529411764705,
      "avg_output_len": 6936.025252525253,
      "avg_power_watts": 954.4093289456731,
      "data_parallel": 1,
      "energy_per_request_joules": 13928.469067483733,
      "energy_per_token_joules": 2.008134134519289,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 16.724804416298866,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 475.271702492195,
      "p90_itl_ms": 17.58667640388012,
      "p95_itl_ms": 18.03741231560707,
      "p99_itl_ms": 18.79562646150588,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.996761658031089,
      "avg_output_len": 6906.156565656565,
      "avg_power_watts": 978.9116740889593,
      "data_parallel": 1,
      "energy_per_request_joules": 8171.951580786593,
      "energy_per_token_joules": 1.1832850157821015,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 19.323566928505898,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 827.2830814492651,
      "p90_itl_ms": 20.279159024357796,
      "p95_itl_ms": 20.528793334960938,
      "p99_itl_ms": 20.97995974123478,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.993795243019648,
      "avg_output_len": 7176.484848484848,
      "avg_power_watts": 1037.5940118925419,
      "data_parallel": 1,
      "energy_per_request_joules": 5753.694684370774,
      "energy_per_token_joules": 0.8017427481346298,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 25.267704389989376,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1294.1732423606625,
      "p90_itl_ms": 27.05940790474415,
      "p95_itl_ms": 27.408520970493555,
      "p99_itl_ms": 27.905541099607944,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 32.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 63.83198380566802,
      "avg_output_len": 7058.378787878788,
      "avg_power_watts": 1114.2135931343146,
      "data_parallel": 1,
      "energy_per_request_joules": 3783.3146125320714,
      "energy_per_token_joules": 0.5360033410262823,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 32.28148818016052,
      "model_id": "Qwen/Qwen3-32B",
      "nickname": "Qwen 3 32B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 2078.7437462627313,
      "p90_itl_ms": 33.90912599861622,
      "p95_itl_ms": 34.28855501115322,
      "p99_itl_ms": 35.215739868581295,
      "tensor_parallel": 1,
      "total_params_billions": 32.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.998084596967279,
      "avg_output_len": 8289.868686868687,
      "avg_power_watts": 293.91033781272114,
      "data_parallel": 1,
      "energy_per_request_joules": 9706.6138467097,
      "energy_per_token_joules": 1.1709007963039468,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 32.49889700964559,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 251.01215981787306,
      "p90_itl_ms": 47.97513481462374,
      "p95_itl_ms": 52.595734488568255,
      "p99_itl_ms": 61.82553002727218,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.995779403489026,
      "avg_output_len": 8201.742424242424,
      "avg_power_watts": 304.31011132656556,
      "data_parallel": 1,
      "energy_per_request_joules": 5854.724917326937,
      "energy_per_token_joules": 0.7138391593500603,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 37.66740398714319,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 426.30066919225237,
      "p90_itl_ms": 52.25049519212916,
      "p95_itl_ms": 57.20704859122633,
      "p99_itl_ms": 67.45306412107313,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.992588017294626,
      "avg_output_len": 7911.282828282829,
      "avg_power_watts": 326.6393169560532,
      "data_parallel": 1,
      "energy_per_request_joules": 3122.321016455263,
      "energy_per_token_joules": 0.3946668428150449,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 38.1577279913472,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 827.6330350587069,
      "p90_itl_ms": 54.285689402604476,
      "p95_itl_ms": 59.819998107559506,
      "p99_itl_ms": 71.81604661076557,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 63.989285714285714,
      "avg_output_len": 8368.025252525253,
      "avg_power_watts": 399.953480739732,
      "data_parallel": 1,
      "energy_per_request_joules": 2194.0490696774477,
      "energy_per_token_joules": 0.26219436527337686,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 41.416758991545066,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1525.4083752819047,
      "p90_itl_ms": 57.11176401237026,
      "p95_itl_ms": 62.85961919347753,
      "p99_itl_ms": 74.74726952263156,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 127.98022598870057,
      "avg_output_len": 8074.010101010101,
      "avg_power_watts": 472.58146393316656,
      "data_parallel": 1,
      "energy_per_request_joules": 1398.175804687691,
      "energy_per_token_joules": 0.17316993503795244,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 47.579443009453826,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2729.0041070327493,
      "p90_itl_ms": 64.1358269873308,
      "p95_itl_ms": 70.02033341268543,
      "p99_itl_ms": 83.70254459936403,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 7.998910081743869,
      "avg_output_len": 7854.186868686868,
      "avg_power_watts": 493.66104975802074,
      "data_parallel": 1,
      "energy_per_request_joules": 4740.496110728974,
      "energy_per_token_joules": 0.6035629391030177,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 9.703529998660088,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 817.9114683411026,
      "p90_itl_ms": 10.597483068704605,
      "p95_itl_ms": 10.925905779004097,
      "p99_itl_ms": 14.275360405445092,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 15.999079189686924,
      "avg_output_len": 7861.343434343435,
      "avg_power_watts": 541.5961996991196,
      "data_parallel": 1,
      "energy_per_request_joules": 3166.0488652766835,
      "energy_per_token_joules": 0.4027363632843381,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 11.912006884813309,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1344.7909080828251,
      "p90_itl_ms": 13.004309311509132,
      "p95_itl_ms": 13.24793053790927,
      "p99_itl_ms": 15.970821734517815,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 31.99470198675497,
      "avg_output_len": 8170.378787878788,
      "avg_power_watts": 608.3187488855558,
      "data_parallel": 1,
      "energy_per_request_joules": 2592.4300686484653,
      "energy_per_token_joules": 0.3172961910278235,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 16.870860010385513,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1917.195245600073,
      "p90_itl_ms": 19.108358770608902,
      "p95_itl_ms": 19.719145260751247,
      "p99_itl_ms": 22.33844097703693,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 63.284351145038165,
      "avg_output_len": 8366.575757575758,
      "avg_power_watts": 669.7547370127364,
      "data_parallel": 1,
      "energy_per_request_joules": 2232.270249679532,
      "energy_per_token_joules": 0.2668081081627998,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 26.641509495675564,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2510.2488137432033,
      "p90_itl_ms": 29.318027198314667,
      "p95_itl_ms": 30.49532836303115,
      "p99_itl_ms": 34.10156220197677,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 8.0,
      "architecture": "Dense Transformer",
      "avg_batch_size": 81.98511904761905,
      "avg_output_len": 8052.388888888889,
      "avg_power_watts": 689.1496984712461,
      "data_parallel": 1,
      "energy_per_request_joules": 1780.0938544970886,
      "energy_per_token_joules": 0.22106406919235558,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 27.875522151589394,
      "model_id": "Qwen/Qwen3-8B",
      "nickname": "Qwen 3 8B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3117.4206689898256,
      "p90_itl_ms": 29.844370484352112,
      "p95_itl_ms": 30.95055818557739,
      "p99_itl_ms": 36.87678739428518,
      "tensor_parallel": 1,
      "total_params_billions": 8.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 7.998855180309102,
      "avg_output_len": 11860.873737373737,
      "avg_power_watts": 4277.775202157113,
      "data_parallel": 1,
      "energy_per_request_joules": 117113.0472451733,
      "energy_per_token_joules": 9.873897137623922,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 18.35748300072737,
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "nickname": "DeepSeek R1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 433.24081085034743,
      "p90_itl_ms": 19.096106800134294,
      "p95_itl_ms": 19.433520199527266,
      "p99_itl_ms": 20.211977198632663,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 15.998377676833226,
      "avg_output_len": 11444.59595959596,
      "avg_power_watts": 4911.310683149415,
      "data_parallel": 1,
      "energy_per_request_joules": 81506.39266134499,
      "energy_per_token_joules": 7.121823518199807,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 23.074202499628882,
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "nickname": "DeepSeek R1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 689.6142077374664,
      "p90_itl_ms": 24.405937698247726,
      "p95_itl_ms": 24.694001998250315,
      "p99_itl_ms": 25.37768642858282,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99740529320187,
      "avg_output_len": 11099.025252525253,
      "avg_power_watts": 5398.11355380596,
      "data_parallel": 1,
      "energy_per_request_joules": 59106.24088812866,
      "energy_per_token_joules": 5.325354212945934,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 31.792588000826072,
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "nickname": "DeepSeek R1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1013.6628171480402,
      "p90_itl_ms": 33.83861699694535,
      "p95_itl_ms": 34.92035549788852,
      "p99_itl_ms": 36.30347400103346,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 63.99813953488372,
      "avg_output_len": 11558.79797979798,
      "avg_power_watts": 6156.141760730251,
      "data_parallel": 1,
      "energy_per_request_joules": 44599.83753683223,
      "energy_per_token_joules": 3.8585186465566834,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 40.92244700223091,
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "nickname": "DeepSeek R1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1595.467671569749,
      "p90_itl_ms": 46.50866350129945,
      "p95_itl_ms": 47.99175275002199,
      "p99_itl_ms": 49.16423429895076,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 127.98820754716981,
      "avg_output_len": 11287.005050505051,
      "avg_power_watts": 7126.160762314122,
      "data_parallel": 1,
      "energy_per_request_joules": 26819.638160958155,
      "energy_per_token_joules": 2.3761518703101916,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 44.80024399526883,
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "nickname": "DeepSeek R1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 2999.034216354128,
      "p90_itl_ms": 55.71015099849319,
      "p95_itl_ms": 57.22191849781666,
      "p99_itl_ms": 70.27125449895033,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 7.999508720216163,
      "avg_output_len": 9081.873737373737,
      "avg_power_watts": 4260.855254483721,
      "data_parallel": 1,
      "energy_per_request_joules": 91299.6181960668,
      "energy_per_token_joules": 10.052949516392252,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 18.759104001219384,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 423.8413062291826,
      "p90_itl_ms": 19.818756598397158,
      "p95_itl_ms": 20.0225453969324,
      "p99_itl_ms": 20.331353237852454,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 15.996767676767677,
      "avg_output_len": 8893.59090909091,
      "avg_power_watts": 4907.697680581875,
      "data_parallel": 1,
      "energy_per_request_joules": 65303.98064629207,
      "energy_per_token_joules": 7.342813641173804,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 23.8196899954346,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 668.3674570007665,
      "p90_itl_ms": 25.742184797127265,
      "p95_itl_ms": 26.12001999950735,
      "p99_itl_ms": 26.771409560169552,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 31.993279569892472,
      "avg_output_len": 9057.924242424242,
      "avg_power_watts": 5364.666963511194,
      "data_parallel": 1,
      "energy_per_request_joules": 49442.84250807956,
      "energy_per_token_joules": 5.458517998694013,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 32.800897999550216,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 982.80649890588,
      "p90_itl_ms": 36.317284997494426,
      "p95_itl_ms": 37.307107500964776,
      "p99_itl_ms": 38.29792009710218,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 63.998511904761905,
      "avg_output_len": 8675.828282828283,
      "avg_power_watts": 6041.29130155796,
      "data_parallel": 1,
      "energy_per_request_joules": 30513.483559207805,
      "energy_per_token_joules": 3.517068637654103,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 37.47100649707136,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 1717.706398129188,
      "p90_itl_ms": 44.22798000450712,
      "p95_itl_ms": 45.16777925164206,
      "p99_itl_ms": 46.848201301327215,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 127.97902097902097,
      "avg_output_len": 9033.636363636364,
      "avg_power_watts": 6159.417027277624,
      "data_parallel": 1,
      "energy_per_request_joules": 16274.998351249935,
      "energy_per_token_joules": 1.8015998979948604,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 39.19294099978288,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 3418.85955596074,
      "p90_itl_ms": 43.72462259925669,
      "p95_itl_ms": 45.05074470034742,
      "p99_itl_ms": 46.65174832058254,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 37.0,
      "architecture": "MoE",
      "avg_batch_size": 255.97607655502392,
      "avg_output_len": 8968.679292929293,
      "avg_power_watts": 6855.990889183953,
      "data_parallel": 1,
      "energy_per_request_joules": 11973.79343495625,
      "energy_per_token_joules": 1.335067633023306,
      "expert_parallel": 8,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 55.46886500087567,
      "model_id": "deepseek-ai/DeepSeek-V3.1",
      "nickname": "DeepSeek V3.1",
      "num_gpus": 8,
      "output_throughput_tokens_per_sec": 5135.31353738112,
      "p90_itl_ms": 60.28186200273922,
      "p95_itl_ms": 62.36090200400213,
      "p99_itl_ms": 86.3645200006431,
      "tensor_parallel": 1,
      "total_params_billions": 671.0,
      "weight_precision": "fp8"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 7.99591211037302,
      "avg_output_len": 5034.651515151515,
      "avg_power_watts": 300.8338981625736,
      "data_parallel": 1,
      "energy_per_request_joules": 6185.8442423478655,
      "energy_per_token_joules": 1.2286539045913898,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 33.29829699941911,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 244.8483637567742,
      "p90_itl_ms": 50.403734209248796,
      "p95_itl_ms": 55.62224350869654,
      "p99_itl_ms": 66.75623906368855,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 15.990814696485623,
      "avg_output_len": 5315.176767676768,
      "avg_power_watts": 294.01141412167726,
      "data_parallel": 1,
      "energy_per_request_joules": 4173.290873302386,
      "energy_per_token_joules": 0.7851650200387422,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 42.41862602066249,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 374.45811596034923,
      "p90_itl_ms": 57.58789060055278,
      "p95_itl_ms": 62.42287452041636,
      "p99_itl_ms": 73.43502314528448,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 31.988066825775658,
      "avg_output_len": 5477.737373737374,
      "avg_power_watts": 341.9688762760672,
      "data_parallel": 1,
      "energy_per_request_joules": 2543.9907644464874,
      "energy_per_token_joules": 0.46442364627473237,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 43.968714991933666,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 736.3295969511717,
      "p90_itl_ms": 58.21801437996328,
      "p95_itl_ms": 63.09183749690418,
      "p99_itl_ms": 74.71878460957672,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 63.959493670886076,
      "avg_output_len": 5208.979797979798,
      "avg_power_watts": 450.3250742846279,
      "data_parallel": 1,
      "energy_per_request_joules": 1289.311710191384,
      "energy_per_token_joules": 0.24751712623101718,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 34.16159101470839,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1819.369354928282,
      "p90_itl_ms": 48.303987813415,
      "p95_itl_ms": 53.22484479111147,
      "p99_itl_ms": 69.65389303863041,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 127.95172413793104,
      "avg_output_len": 5362.747474747474,
      "avg_power_watts": 617.5188043427743,
      "data_parallel": 1,
      "energy_per_request_joules": 958.3800388815604,
      "energy_per_token_joules": 0.17871064102765521,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 37.741620995802805,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3455.4115009145657,
      "p90_itl_ms": 51.527896503102966,
      "p95_itl_ms": 56.18451876944164,
      "p99_itl_ms": 72.05681301275035,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 255.8941798941799,
      "avg_output_len": 5646.025252525253,
      "avg_power_watts": 839.8617236905329,
      "data_parallel": 1,
      "energy_per_request_joules": 820.2135258586491,
      "energy_per_token_joules": 0.14527273421099182,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 44.9442325043492,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 5781.275669188763,
      "p90_itl_ms": 58.51926428731531,
      "p95_itl_ms": 63.66154285351514,
      "p99_itl_ms": 88.49739996599965,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 7.988474576271186,
      "avg_output_len": 5363.777777777777,
      "avg_power_watts": 446.0593128436694,
      "data_parallel": 1,
      "energy_per_request_joules": 3535.0226739370787,
      "energy_per_token_joules": 0.6590546477489686,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 11.661284603178501,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 676.8168836487315,
      "p90_itl_ms": 12.108508683741093,
      "p95_itl_ms": 12.212002836167812,
      "p99_itl_ms": 15.957467816770075,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 15.99030303030303,
      "avg_output_len": 5401.813131313132,
      "avg_power_watts": 472.11801599143433,
      "data_parallel": 1,
      "energy_per_request_joules": 2112.0013654977292,
      "energy_per_token_joules": 0.39098008652963545,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 13.13377171754837,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1207.5244552273348,
      "p90_itl_ms": 13.455495238304138,
      "p95_itl_ms": 13.61302100121975,
      "p99_itl_ms": 17.528010904788967,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 31.975717439293597,
      "avg_output_len": 5399.621212121212,
      "avg_power_watts": 502.2718035856321,
      "data_parallel": 1,
      "energy_per_request_joules": 1350.1045840000231,
      "energy_per_token_joules": 0.25003690647211935,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 15.695895999670029,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2008.7906648358667,
      "p90_itl_ms": 16.001389920711517,
      "p95_itl_ms": 16.2830812856555,
      "p99_itl_ms": 20.27042657136917,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 63.957692307692305,
      "avg_output_len": 5584.555555555556,
      "avg_power_watts": 549.5660860110509,
      "data_parallel": 1,
      "energy_per_request_joules": 1034.3796153664746,
      "energy_per_token_joules": 0.1852214746681974,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 20.89649997651577,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2967.075426839863,
      "p90_itl_ms": 21.905543468892574,
      "p95_itl_ms": 22.470758948475122,
      "p99_itl_ms": 26.676567345857627,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 95.94252873563218,
      "avg_output_len": 5259.707070707071,
      "avg_power_watts": 578.6646866567021,
      "data_parallel": 1,
      "energy_per_request_joules": 837.5962507767173,
      "energy_per_token_joules": 0.1592476994472846,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 24.93886649608612,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3633.739693980673,
      "p90_itl_ms": 27.007137052714825,
      "p95_itl_ms": 28.28009370714426,
      "p99_itl_ms": 34.73140273243189,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 127.98540145985402,
      "avg_output_len": 5695.111111111111,
      "avg_power_watts": 578.9337210252614,
      "data_parallel": 1,
      "energy_per_request_joules": 821.5363080255328,
      "energy_per_token_joules": 0.14425290253296774,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 30.360515229403973,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4013.324590768295,
      "p90_itl_ms": 32.88780227303505,
      "p95_itl_ms": 34.272380080074065,
      "p99_itl_ms": 38.53832149878139,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 12.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 192.0,
      "avg_output_len": 5767.858585858586,
      "avg_power_watts": 605.4664177961711,
      "data_parallel": 1,
      "energy_per_request_joules": 723.1375341325939,
      "energy_per_token_joules": 0.12537365876229262,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 192,
      "median_itl_ms": 36.748494021594524,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
      "nickname": "NVIDIA Nemotron Nano 12B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4829.295274409517,
      "p90_itl_ms": 40.24399183690548,
      "p95_itl_ms": 40.85724940523505,
      "p99_itl_ms": 44.18877232819786,
      "tensor_parallel": 1,
      "total_params_billions": 12.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 7.99564649542882,
      "avg_output_len": 5254.106060606061,
      "avg_power_watts": 352.67216243039917,
      "data_parallel": 1,
      "energy_per_request_joules": 4306.510145816889,
      "energy_per_token_joules": 0.8196465956608674,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 17.254578007850796,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 430.2734426000336,
      "p90_itl_ms": 31.70320179779082,
      "p95_itl_ms": 36.6122944978997,
      "p99_itl_ms": 46.534189914818846,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 15.986198243412797,
      "avg_output_len": 5723.282828282829,
      "avg_power_watts": 340.6810701125498,
      "data_parallel": 1,
      "energy_per_request_joules": 2983.29065893396,
      "energy_per_token_joules": 0.5212551517096778,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 23.126269501517527,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 653.5783272263908,
      "p90_itl_ms": 37.65707430429758,
      "p95_itl_ms": 42.44033214490628,
      "p99_itl_ms": 53.19268749764874,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 31.986577181208055,
      "avg_output_len": 5339.146464646465,
      "avg_power_watts": 376.7544981082948,
      "data_parallel": 1,
      "energy_per_request_joules": 1610.787831930311,
      "energy_per_token_joules": 0.30169388358162796,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 24.657885980559513,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1248.797269721108,
      "p90_itl_ms": 38.339399400865666,
      "p95_itl_ms": 43.069945805473175,
      "p99_itl_ms": 54.09749420243314,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 63.9719387755102,
      "avg_output_len": 5559.823232323232,
      "avg_power_watts": 454.59715473092245,
      "data_parallel": 1,
      "energy_per_request_joules": 1229.8968813085517,
      "energy_per_token_joules": 0.22121150797713868,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 29.164551000576466,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2055.033930594168,
      "p90_itl_ms": 45.27728440007195,
      "p95_itl_ms": 50.837154596229055,
      "p99_itl_ms": 67.88686528278053,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 127.93582887700535,
      "avg_output_len": 5562.626262626262,
      "avg_power_watts": 530.4339142801433,
      "data_parallel": 1,
      "energy_per_request_joules": 887.2613135915349,
      "energy_per_token_joules": 0.15950403131571084,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 37.856785507756285,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3325.5204266921655,
      "p90_itl_ms": 53.07521491777152,
      "p95_itl_ms": 58.64271321188425,
      "p99_itl_ms": 79.10111178702209,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 255.92244897959185,
      "avg_output_len": 5603.497474747474,
      "avg_power_watts": 686.8867989048252,
      "data_parallel": 1,
      "energy_per_request_joules": 741.1076968993503,
      "energy_per_token_joules": 0.1322580585142048,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 45.075016998453066,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 5193.534568867515,
      "p90_itl_ms": 73.60894620069303,
      "p95_itl_ms": 82.9855680058245,
      "p99_itl_ms": 105.91077668126681,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 511.936,
      "avg_output_len": 5361.003367003367,
      "avg_power_watts": 923.2141960719636,
      "data_parallel": 1,
      "energy_per_request_joules": 625.8982625484965,
      "energy_per_token_joules": 0.11675020881368221,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 72.19255898962729,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 7907.60209727154,
      "p90_itl_ms": 98.27076219080482,
      "p95_itl_ms": 106.4502715220441,
      "p99_itl_ms": 129.05968089733503,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 7.986334405144695,
      "avg_output_len": 5412.040404040404,
      "avg_power_watts": 401.99450485610333,
      "data_parallel": 1,
      "energy_per_request_joules": 2616.031550940964,
      "energy_per_token_joules": 0.48337250937519677,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 9.505512192845345,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 831.6453605847756,
      "p90_itl_ms": 9.730427525937557,
      "p95_itl_ms": 9.785777889192104,
      "p99_itl_ms": 13.753928616642943,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 15.986666666666666,
      "avg_output_len": 5429.767676767677,
      "avg_power_watts": 434.988344426694,
      "data_parallel": 1,
      "energy_per_request_joules": 1588.1984090480598,
      "energy_per_token_joules": 0.29249840943351546,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 10.604096576571465,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1487.1477259282888,
      "p90_itl_ms": 10.849873535335064,
      "p95_itl_ms": 10.94916881993413,
      "p99_itl_ms": 15.016931947320701,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 31.977272727272727,
      "avg_output_len": 5513.328282828283,
      "avg_power_watts": 475.5057491661098,
      "data_parallel": 1,
      "energy_per_request_joules": 1091.6850694513605,
      "energy_per_token_joules": 0.19800835601455186,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 13.077691197395325,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2401.4428417918098,
      "p90_itl_ms": 13.414505869150162,
      "p95_itl_ms": 13.77813145518303,
      "p99_itl_ms": 17.646052315831202,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 63.97899159663866,
      "avg_output_len": 5385.434343434344,
      "avg_power_watts": 518.0810079474549,
      "data_parallel": 1,
      "energy_per_request_joules": 812.9753894484143,
      "energy_per_token_joules": 0.1509581841694076,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 18.010043539106846,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3431.950449046581,
      "p90_itl_ms": 19.111029617488384,
      "p95_itl_ms": 20.565940439701077,
      "p99_itl_ms": 23.601431734860018,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 95.94827586206897,
      "avg_output_len": 5557.171717171717,
      "avg_power_watts": 549.4045287832402,
      "data_parallel": 1,
      "energy_per_request_joules": 740.2723833010303,
      "energy_per_token_joules": 0.13321027691362877,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 22.324533201754093,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4124.340415112751,
      "p90_itl_ms": 23.706649988889694,
      "p95_itl_ms": 24.716751277446747,
      "p99_itl_ms": 30.508321672678008,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 127.97674418604652,
      "avg_output_len": 5325.550505050505,
      "avg_power_watts": 549.1126313203853,
      "data_parallel": 1,
      "energy_per_request_joules": 648.2202367462521,
      "energy_per_token_joules": 0.1217189164071414,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 27.26638689637184,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4511.317119219508,
      "p90_itl_ms": 28.777630999684334,
      "p95_itl_ms": 29.215967282652855,
      "p99_itl_ms": 34.23901721835138,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 191.9486301369863,
      "avg_output_len": 5502.373737373738,
      "avg_power_watts": 555.1837410340161,
      "data_parallel": 1,
      "energy_per_request_joules": 635.8095844596141,
      "energy_per_token_joules": 0.11555187175691262,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 192,
      "median_itl_ms": 38.75891678035259,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4804.627848884702,
      "p90_itl_ms": 49.0272868424654,
      "p95_itl_ms": 51.12214758992195,
      "p99_itl_ms": 82.37348513677712,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 9.0,
      "architecture": "Mamba-Transformer Hybrid",
      "avg_batch_size": 255.94372294372295,
      "avg_output_len": 5530.828282828283,
      "avg_power_watts": 585.6974121900956,
      "data_parallel": 1,
      "energy_per_request_joules": 602.4472129303057,
      "energy_per_token_joules": 0.10892531500222859,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 256,
      "median_itl_ms": 47.55112063139677,
      "model_id": "nvidia/NVIDIA-Nemotron-Nano-9B-v2",
      "nickname": "NVIDIA Nemotron Nano 9B V2",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 5377.055023234153,
      "p90_itl_ms": 56.970587000250816,
      "p95_itl_ms": 60.44964473694563,
      "p99_itl_ms": 84.74351633340126,
      "tensor_parallel": 1,
      "total_params_billions": 9.0,
      "weight_precision": "bfloat16"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 7.991869918699187,
      "avg_output_len": 1746.5757575757575,
      "avg_power_watts": 659.8314270319853,
      "data_parallel": 1,
      "energy_per_request_joules": 848.6438320368941,
      "energy_per_token_joules": 0.48589007854707056,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 5.45397300447803,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1357.9849767771377,
      "p90_itl_ms": 7.809739495860412,
      "p95_itl_ms": 10.606128258223157,
      "p99_itl_ms": 15.62396485416682,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 15.974358974358974,
      "avg_output_len": 1795.878787878788,
      "avg_power_watts": 699.6688036032991,
      "data_parallel": 1,
      "energy_per_request_joules": 594.504510413729,
      "energy_per_token_joules": 0.331038216179351,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 6.901834000018425,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2113.5590074114893,
      "p90_itl_ms": 12.178540087188598,
      "p95_itl_ms": 13.999344779585952,
      "p99_itl_ms": 19.26879448117685,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99074074074074,
      "avg_output_len": 1776.040404040404,
      "avg_power_watts": 717.9210601086755,
      "data_parallel": 1,
      "energy_per_request_joules": 458.5115026304264,
      "energy_per_token_joules": 0.258165017860706,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 10.07053600915242,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2780.8611176593754,
      "p90_itl_ms": 17.513591202441606,
      "p95_itl_ms": 20.429867868369914,
      "p99_itl_ms": 26.98967366071887,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 64.0,
      "avg_output_len": 1738.540404040404,
      "avg_power_watts": 762.2634694014198,
      "data_parallel": 1,
      "energy_per_request_joules": 314.0181877814504,
      "energy_per_token_joules": 0.18062173709145077,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 13.951387998531573,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4220.220011589621,
      "p90_itl_ms": 22.22058200277388,
      "p95_itl_ms": 25.4594292564434,
      "p99_itl_ms": 32.2278789491975,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 127.92647058823529,
      "avg_output_len": 1815.2222222222222,
      "avg_power_watts": 855.147302556861,
      "data_parallel": 1,
      "energy_per_request_joules": 211.29297581733644,
      "energy_per_token_joules": 0.11640061102748535,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 16.48175199807156,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 7346.587745617052,
      "p90_itl_ms": 24.28926430293359,
      "p95_itl_ms": 27.64103554654866,
      "p99_itl_ms": 34.404707649373435,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 255.83870967741936,
      "avg_output_len": 1861.2609427609427,
      "avg_power_watts": 952.0882696308249,
      "data_parallel": 1,
      "energy_per_request_joules": 149.05712431983224,
      "energy_per_token_joules": 0.08008394787392092,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 20.717412000522017,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 11888.628057269756,
      "p90_itl_ms": 25.696520385099575,
      "p95_itl_ms": 27.278679786832072,
      "p99_itl_ms": 31.382965607917868,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 511.8181818181818,
      "avg_output_len": 1832.3977272727273,
      "avg_power_watts": 893.2711214812631,
      "data_parallel": 1,
      "energy_per_request_joules": 101.28340004840888,
      "energy_per_token_joules": 0.05527369879417791,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 30.294244992546737,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 16160.871100874348,
      "p90_itl_ms": 39.1099724045489,
      "p95_itl_ms": 41.19197288819123,
      "p99_itl_ms": 46.34756007755641,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 1023.5641025641025,
      "avg_output_len": 1800.9781144781145,
      "avg_power_watts": 779.0159052431786,
      "data_parallel": 1,
      "energy_per_request_joules": 78.97161895065982,
      "energy_per_token_joules": 0.04384929406737634,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 1024,
      "median_itl_ms": 47.406072000740096,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 17765.757050642296,
      "p90_itl_ms": 74.54672059975564,
      "p95_itl_ms": 91.96001079981212,
      "p99_itl_ms": 114.10652056219992,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 2047.3855421686746,
      "avg_output_len": 1808.5206228956229,
      "avg_power_watts": 713.4358023979836,
      "data_parallel": 1,
      "energy_per_request_joules": 76.08211162305982,
      "energy_per_token_joules": 0.042068700052336,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 2048,
      "median_itl_ms": 130.05510000220966,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 16958.82690718816,
      "p90_itl_ms": 181.18592360115147,
      "p95_itl_ms": 191.36647525156147,
      "p99_itl_ms": 218.87476456176947,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 2885.2268907563025,
      "avg_output_len": 1793.6930415263748,
      "avg_power_watts": 673.5046499567148,
      "data_parallel": 1,
      "energy_per_request_joules": 71.14999922002823,
      "energy_per_token_joules": 0.0396667643642537,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 3072,
      "median_itl_ms": 175.4405300016515,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 16979.06700360097,
      "p90_itl_ms": 210.95160601544194,
      "p95_itl_ms": 225.76437098905444,
      "p99_itl_ms": 319.45279978972377,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 8.0,
      "avg_output_len": 1565.550505050505,
      "avg_power_watts": 1067.9081734775832,
      "data_parallel": 1,
      "energy_per_request_joules": 982.7316614532481,
      "energy_per_token_joules": 0.6277227456303269,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 4.657438985304907,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1701.2417996821905,
      "p90_itl_ms": 4.903670208295807,
      "p95_itl_ms": 5.0160094047896555,
      "p99_itl_ms": 5.42817416368052,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 15.981481481481481,
      "avg_output_len": 1636.8080808080808,
      "avg_power_watts": 1207.9932870816103,
      "data_parallel": 1,
      "energy_per_request_joules": 706.148482337234,
      "energy_per_token_joules": 0.43141800838899413,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 5.63221650372725,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 2800.05299637934,
      "p90_itl_ms": 5.898638404323719,
      "p95_itl_ms": 6.022722747002263,
      "p99_itl_ms": 7.582832919433715,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 31.921875,
      "avg_output_len": 1658.5151515151515,
      "avg_power_watts": 1355.6628027133322,
      "data_parallel": 1,
      "energy_per_request_joules": 497.9539705974188,
      "energy_per_token_joules": 0.30024083297792514,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 6.954117503482848,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 4515.25127101218,
      "p90_itl_ms": 7.299648894695566,
      "p95_itl_ms": 7.457647251430899,
      "p99_itl_ms": 13.230332161183485,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 63.91428571428571,
      "avg_output_len": 1559.909090909091,
      "avg_power_watts": 1462.743173222873,
      "data_parallel": 1,
      "energy_per_request_joules": 321.8872253306239,
      "energy_per_token_joules": 0.20634999001322119,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 8.798983006272465,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 7088.651533877722,
      "p90_itl_ms": 9.287583603872918,
      "p95_itl_ms": 9.778764459770171,
      "p99_itl_ms": 16.495890735241122,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 127.93877551020408,
      "avg_output_len": 1760.0126262626263,
      "avg_power_watts": 1517.7596917099556,
      "data_parallel": 1,
      "energy_per_request_joules": 260.03008309820024,
      "energy_per_token_joules": 0.14774330548433176,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 12.117908016080037,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 10272.95068791401,
      "p90_itl_ms": 13.056740420870485,
      "p95_itl_ms": 14.86866000341251,
      "p99_itl_ms": 18.686776203103364,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 255.8409090909091,
      "avg_output_len": 1676.577441077441,
      "avg_power_watts": 1658.658721130682,
      "data_parallel": 1,
      "energy_per_request_joules": 177.555715877408,
      "energy_per_token_joules": 0.10590367705490719,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 15.680338983656839,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 15661.955913681146,
      "p90_itl_ms": 19.89219079259783,
      "p95_itl_ms": 21.525613620178767,
      "p99_itl_ms": 25.227223421679767,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 511.8787878787879,
      "avg_output_len": 1673.0694444444443,
      "avg_power_watts": 1642.7936294692759,
      "data_parallel": 1,
      "energy_per_request_joules": 136.5259777997511,
      "energy_per_token_joules": 0.08160209861766116,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 24.378373025683686,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 20131.757115297103,
      "p90_itl_ms": 30.872450990136716,
      "p95_itl_ms": 32.35608909744769,
      "p99_itl_ms": 38.31311052374076,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 1023.6326530612245,
      "avg_output_len": 1632.7005772005773,
      "avg_power_watts": 1512.14937374798,
      "data_parallel": 1,
      "energy_per_request_joules": 117.38979535944479,
      "energy_per_token_joules": 0.07189915713799827,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 1024,
      "median_itl_ms": 40.83580899168737,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 21031.531299395694,
      "p90_itl_ms": 65.67976798978636,
      "p95_itl_ms": 75.68022679188287,
      "p99_itl_ms": 85.90234324452462,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 2047.27868852459,
      "avg_output_len": 1650.9819023569023,
      "avg_power_watts": 1472.5107613760902,
      "data_parallel": 1,
      "energy_per_request_joules": 108.47488054469393,
      "energy_per_token_joules": 0.06570325234324967,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 2048,
      "median_itl_ms": 88.74269849911798,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 22411.535332883344,
      "p90_itl_ms": 155.07174840313382,
      "p95_itl_ms": 176.24314289423634,
      "p99_itl_ms": 229.7573606693185,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 3071.1685393258426,
      "avg_output_len": 1641.5496632996633,
      "avg_power_watts": 1327.4665650816123,
      "data_parallel": 1,
      "energy_per_request_joules": 99.88134975383628,
      "energy_per_token_joules": 0.06084576786611849,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 3072,
      "median_itl_ms": 158.94596549333073,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 21816.908745444598,
      "p90_itl_ms": 217.16104648658074,
      "p95_itl_ms": 261.172524507856,
      "p99_itl_ms": 430.8699949106085,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 4095.0243902439024,
      "avg_output_len": 1634.4890572390573,
      "avg_power_watts": 1301.8971482487527,
      "data_parallel": 1,
      "energy_per_request_joules": 99.41804481833107,
      "energy_per_token_joules": 0.06082515167539012,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 4096,
      "median_itl_ms": 208.13817699672654,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 21403.92769091114,
      "p90_itl_ms": 291.7018283129438,
      "p95_itl_ms": 378.0616642499807,
      "p99_itl_ms": 633.2739127724216,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 7.992840095465394,
      "avg_output_len": 1780.6969696969697,
      "avg_power_watts": 549.9648189244016,
      "data_parallel": 1,
      "energy_per_request_joules": 1218.1369682135755,
      "energy_per_token_joules": 0.6840787562079539,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 9.85749065876007,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 803.9495656509134,
      "p90_itl_ms": 10.23793164640665,
      "p95_itl_ms": 10.347421746701002,
      "p99_itl_ms": 10.778715033084152,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 15.99622641509434,
      "avg_output_len": 1679.989898989899,
      "avg_power_watts": 609.8622051217071,
      "data_parallel": 1,
      "energy_per_request_joules": 859.9462870351178,
      "energy_per_token_joules": 0.5118758675585872,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 13.226334936916828,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1191.4259760485872,
      "p90_itl_ms": 13.8166768476367,
      "p95_itl_ms": 14.032936654984951,
      "p99_itl_ms": 18.462037704885013,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 31.98857142857143,
      "avg_output_len": 1757.919191919192,
      "avg_power_watts": 655.9374048511457,
      "data_parallel": 1,
      "energy_per_request_joules": 660.8237144790817,
      "energy_per_token_joules": 0.3759124523566032,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 17.96223595738411,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1744.920661018437,
      "p90_itl_ms": 18.692253343760967,
      "p95_itl_ms": 19.1040492306153,
      "p99_itl_ms": 35.42223773896699,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 64.0,
      "avg_output_len": 1738.8131313131314,
      "avg_power_watts": 688.1701637002966,
      "data_parallel": 1,
      "energy_per_request_joules": 465.2560728226024,
      "energy_per_token_joules": 0.26757106007777065,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 24.15146678686142,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2571.915525917777,
      "p90_itl_ms": 25.258196517825127,
      "p95_itl_ms": 26.005130261182785,
      "p99_itl_ms": 42.64888353645794,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 7.996644295302014,
      "avg_output_len": 1760.1818181818182,
      "avg_power_watts": 864.1739530037215,
      "data_parallel": 1,
      "energy_per_request_joules": 1376.3286549099437,
      "energy_per_token_joules": 0.7819241402752495,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 7.176676765084267,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1105.1889927576847,
      "p90_itl_ms": 7.405425980687141,
      "p95_itl_ms": 7.466542534530163,
      "p99_itl_ms": 7.698871921747921,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 15.983783783783784,
      "avg_output_len": 1715.3838383838383,
      "avg_power_watts": 992.4357622410904,
      "data_parallel": 1,
      "energy_per_request_joules": 991.4288558979787,
      "energy_per_token_joules": 0.5779632719590391,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 9.206450544297695,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 1717.1260015834112,
      "p90_itl_ms": 9.507095068693161,
      "p95_itl_ms": 9.606552217155695,
      "p99_itl_ms": 10.510198529809712,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 31.99145299145299,
      "avg_output_len": 1773.5353535353536,
      "avg_power_watts": 1086.5274011846657,
      "data_parallel": 1,
      "energy_per_request_joules": 739.2842808764685,
      "energy_per_token_joules": 0.4168421449297778,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 12.075122445821762,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 2606.568012377215,
      "p90_itl_ms": 12.440547347068787,
      "p95_itl_ms": 12.622483540326357,
      "p99_itl_ms": 21.764685586094853,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 63.92537313432836,
      "avg_output_len": 1680.7676767676767,
      "avg_power_watts": 1164.007045700212,
      "data_parallel": 1,
      "energy_per_request_joules": 500.85595770715975,
      "energy_per_token_joules": 0.2979923785007381,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 16.037579625844955,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 3906.163813841731,
      "p90_itl_ms": 16.466877050697803,
      "p95_itl_ms": 16.68805722147226,
      "p99_itl_ms": 25.734602697193623,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 95.93023255813954,
      "avg_output_len": 1807.7828282828282,
      "avg_power_watts": 1212.7099843565145,
      "data_parallel": 1,
      "energy_per_request_joules": 451.8964701482083,
      "energy_per_token_joules": 0.24997276391736417,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 19.24681104719639,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 4851.368466515861,
      "p90_itl_ms": 19.789204001426697,
      "p95_itl_ms": 20.003909431397915,
      "p99_itl_ms": 27.672958895564076,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 127.96666666666667,
      "avg_output_len": 1756.489898989899,
      "avg_power_watts": 1208.3604693401549,
      "data_parallel": 1,
      "energy_per_request_joules": 391.0497314693555,
      "energy_per_token_joules": 0.22263135796809058,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 22.496696561574936,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 5427.62924490335,
      "p90_itl_ms": 23.41281995177269,
      "p95_itl_ms": 23.590007796883583,
      "p99_itl_ms": 30.726888962089998,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 191.87692307692308,
      "avg_output_len": 1733.1060606060605,
      "avg_power_watts": 1264.511861931454,
      "data_parallel": 1,
      "energy_per_request_joules": 339.77115382721445,
      "energy_per_token_joules": 0.19604752504783104,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 192,
      "median_itl_ms": 29.22416350338608,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 6450.027163683613,
      "p90_itl_ms": 30.091804104449693,
      "p95_itl_ms": 30.722570798388915,
      "p99_itl_ms": 34.1212665657804,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 255.8139534883721,
      "avg_output_len": 1690.6439393939395,
      "avg_power_watts": 1238.1363237302414,
      "data_parallel": 1,
      "energy_per_request_joules": 254.57142236133882,
      "energy_per_token_joules": 0.1505766036416854,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 256,
      "median_itl_ms": 30.520553002133965,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 8222.63415288959,
      "p90_itl_ms": 31.859954996616576,
      "p95_itl_ms": 32.78333039197605,
      "p99_itl_ms": 38.00529548199845,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 383.9056603773585,
      "avg_output_len": 1731.1919191919192,
      "avg_power_watts": 1258.8980713979674,
      "data_parallel": 1,
      "energy_per_request_joules": 234.53566288665868,
      "energy_per_token_joules": 0.13547640806695457,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 384,
      "median_itl_ms": 40.18088299926603,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 9292.378572480311,
      "p90_itl_ms": 42.15094749815762,
      "p95_itl_ms": 43.16567475325428,
      "p99_itl_ms": 45.19459044677207,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 511.72058823529414,
      "avg_output_len": 1747.6527777777778,
      "avg_power_watts": 1163.858988838859,
      "data_parallel": 1,
      "energy_per_request_joules": 200.53267832077648,
      "energy_per_token_joules": 0.11474400457038335,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 512,
      "median_itl_ms": 48.84157798369415,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 10143.091947997633,
      "p90_itl_ms": 51.67187621118501,
      "p95_itl_ms": 53.012487810337916,
      "p99_itl_ms": 60.52728712093086,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 767.75,
      "avg_output_len": 1718.7171717171718,
      "avg_power_watts": 1169.4075889437233,
      "data_parallel": 1,
      "energy_per_request_joules": 177.10918409911557,
      "energy_per_token_joules": 0.10304731168896487,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 768,
      "median_itl_ms": 65.03442699613515,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 11348.259064471576,
      "p90_itl_ms": 69.38796988979448,
      "p95_itl_ms": 71.45622423995519,
      "p99_itl_ms": 96.55798725638303,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 1023.7627118644068,
      "avg_output_len": 1775.2390572390573,
      "avg_power_watts": 1138.9522207481266,
      "data_parallel": 1,
      "energy_per_request_joules": 169.35816192856834,
      "energy_per_token_joules": 0.09540020046199459,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 1024,
      "median_itl_ms": 79.71355800691526,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 11938.677437075836,
      "p90_itl_ms": 87.2380607004743,
      "p95_itl_ms": 91.536516979977,
      "p99_itl_ms": 114.61825579055589,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 1535.6063829787233,
      "avg_output_len": 1716.2480359147025,
      "avg_power_watts": 973.2574856315339,
      "data_parallel": 1,
      "energy_per_request_joules": 151.16576156145595,
      "energy_per_token_joules": 0.08807920440292867,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 1536,
      "median_itl_ms": 126.29272416234016,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 11049.798783141287,
      "p90_itl_ms": 142.54298806190496,
      "p95_itl_ms": 152.40283245220778,
      "p99_itl_ms": 196.24773861840376,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 5.0,
      "architecture": "MoE",
      "avg_batch_size": 1915.4583333333333,
      "avg_output_len": 1720.965909090909,
      "avg_power_watts": 925.9872288388464,
      "data_parallel": 1,
      "energy_per_request_joules": 148.07166948273303,
      "energy_per_token_joules": 0.08603986209171981,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 2048,
      "median_itl_ms": 122.57824651896954,
      "model_id": "openai/gpt-oss-120b",
      "nickname": "GPT OSS 120B",
      "num_gpus": 2,
      "output_throughput_tokens_per_sec": 10762.30489365185,
      "p90_itl_ms": 174.04294423758984,
      "p95_itl_ms": 190.80506693571806,
      "p99_itl_ms": 271.0348512977364,
      "tensor_parallel": 1,
      "total_params_billions": 117.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 7.991379310344827,
      "avg_output_len": 2117.348484848485,
      "avg_power_watts": 487.34635115324164,
      "data_parallel": 1,
      "energy_per_request_joules": 620.2208058584154,
      "energy_per_token_joules": 0.29292334743035825,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 8,
      "median_itl_ms": 3.8020270003471524,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1663.7333808603528,
      "p90_itl_ms": 9.665562404552476,
      "p95_itl_ms": 13.152091996744264,
      "p99_itl_ms": 21.139187243534284,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 15.984848484848484,
      "avg_output_len": 2069.6414141414143,
      "avg_power_watts": 513.785257446052,
      "data_parallel": 1,
      "energy_per_request_joules": 384.04394767207856,
      "energy_per_token_joules": 0.1855606217811399,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 16,
      "median_itl_ms": 4.509860009420663,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2768.8269877217685,
      "p90_itl_ms": 10.367306612897664,
      "p95_itl_ms": 12.821885108132841,
      "p99_itl_ms": 17.887397630838677,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 31.971830985915492,
      "avg_output_len": 1979.4292929292928,
      "avg_power_watts": 538.147180607641,
      "data_parallel": 1,
      "energy_per_request_joules": 232.59757683107844,
      "energy_per_token_joules": 0.11750739350071196,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 32,
      "median_itl_ms": 5.727601004764438,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 4579.68783559462,
      "p90_itl_ms": 11.368510196916759,
      "p95_itl_ms": 13.393376593012361,
      "p99_itl_ms": 18.13099972088821,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 63.926829268292686,
      "avg_output_len": 2093.4646464646466,
      "avg_power_watts": 563.355340144207,
      "data_parallel": 1,
      "energy_per_request_joules": 167.1883939370373,
      "energy_per_token_joules": 0.07986205748417002,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 64,
      "median_itl_ms": 7.655216002603993,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 7054.105014209949,
      "p90_itl_ms": 14.44086320407223,
      "p95_itl_ms": 16.68027670384617,
      "p99_itl_ms": 21.58955227700063,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 127.92307692307692,
      "avg_output_len": 2163.787878787879,
      "avg_power_watts": 503.0141640098874,
      "data_parallel": 1,
      "energy_per_request_joules": 142.2952229972285,
      "energy_per_token_joules": 0.06576209451590981,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 128,
      "median_itl_ms": 16.269230996840633,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 7648.998525863455,
      "p90_itl_ms": 25.47600100806449,
      "p95_itl_ms": 29.14400730514898,
      "p99_itl_ms": 37.37342791602716,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 255.83870967741936,
      "avg_output_len": 2112.240740740741,
      "avg_power_watts": 428.61381219910135,
      "data_parallel": 1,
      "energy_per_request_joules": 110.89749708649097,
      "energy_per_token_joules": 0.05250230002078284,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 256,
      "median_itl_ms": 28.6903619999066,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 8163.714961619513,
      "p90_itl_ms": 43.16039238474332,
      "p95_itl_ms": 47.189951204927624,
      "p99_itl_ms": 56.821500119985984,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 383.8888888888889,
      "avg_output_len": 2149.1296296296296,
      "avg_power_watts": 632.425119864397,
      "data_parallel": 1,
      "energy_per_request_joules": 84.46903414526743,
      "energy_per_token_joules": 0.03930383397106875,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 384,
      "median_itl_ms": 23.48228602204472,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 16090.672485791596,
      "p90_itl_ms": 31.958975974703208,
      "p95_itl_ms": 34.95753000606783,
      "p99_itl_ms": 42.108738009119406,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 511.796875,
      "avg_output_len": 2151.2070707070707,
      "avg_power_watts": 466.76314556477615,
      "data_parallel": 1,
      "energy_per_request_joules": 87.45116663312852,
      "energy_per_token_joules": 0.040652137966608944,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 512,
      "median_itl_ms": 39.73364799458068,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 11481.884321758633,
      "p90_itl_ms": 55.37156941718422,
      "p95_itl_ms": 60.71473978081483,
      "p99_itl_ms": 73.14953174296534,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 1023.675,
      "avg_output_len": 2158.526936026936,
      "avg_power_watts": 583.481621181594,
      "data_parallel": 1,
      "energy_per_request_joules": 68.5366296623726,
      "energy_per_token_joules": 0.031751574890477696,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 1024,
      "median_itl_ms": 44.50300549797248,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 18376.462370582456,
      "p90_itl_ms": 64.08914560161067,
      "p95_itl_ms": 73.77117075666317,
      "p99_itl_ms": 94.47647912660598,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 2047.6190476190477,
      "avg_output_len": 2087.5413223140495,
      "avg_power_watts": 646.0116180331404,
      "data_parallel": 1,
      "energy_per_request_joules": 60.739727076582945,
      "energy_per_token_joules": 0.029096299281516816,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 2048,
      "median_itl_ms": 100.45418801018968,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 22202.53551088244,
      "p90_itl_ms": 154.7147459932603,
      "p95_itl_ms": 168.74005200224929,
      "p99_itl_ms": 202.7145073952852,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 4094.6792452830186,
      "avg_output_len": 2144.7462722462724,
      "avg_power_watts": 611.6523805816164,
      "data_parallel": 1,
      "energy_per_request_joules": 61.010607460662435,
      "energy_per_token_joules": 0.028446538525400376,
      "expert_parallel": 1,
      "gpu_model": "B200",
      "max_num_seqs": 4096,
      "median_itl_ms": 247.04576301155612,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 21501.821040034873,
      "p90_itl_ms": 302.44145499309525,
      "p95_itl_ms": 320.59122440405184,
      "p99_itl_ms": 496.94883844233124,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 7.993808049535604,
      "avg_output_len": 2142.40404040404,
      "avg_power_watts": 436.8165428008319,
      "data_parallel": 1,
      "energy_per_request_joules": 763.7174126606158,
      "energy_per_token_joules": 0.35647683548831655,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 8,
      "median_itl_ms": 6.6473519982537255,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 1225.371466851311,
      "p90_itl_ms": 7.106321495666634,
      "p95_itl_ms": 7.344038711016764,
      "p99_itl_ms": 8.563895427942033,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 15.989473684210527,
      "avg_output_len": 2144.8636363636365,
      "avg_power_watts": 472.01389388968306,
      "data_parallel": 1,
      "energy_per_request_joules": 493.62585391768636,
      "energy_per_token_joules": 0.2301432340727128,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 16,
      "median_itl_ms": 8.021378991543315,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 2050.957073717632,
      "p90_itl_ms": 8.339498200803064,
      "p95_itl_ms": 8.485308394301683,
      "p99_itl_ms": 12.094341671327124,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 31.97222222222222,
      "avg_output_len": 2183.439393939394,
      "avg_power_watts": 530.8482485818803,
      "data_parallel": 1,
      "energy_per_request_joules": 341.1485438687084,
      "energy_per_token_joules": 0.15624365155984618,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 32,
      "median_itl_ms": 9.244729997590184,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 3397.5668341222104,
      "p90_itl_ms": 9.749028403894044,
      "p95_itl_ms": 10.465670099074487,
      "p99_itl_ms": 14.680375504831316,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 63.94444444444444,
      "avg_output_len": 2091.2979797979797,
      "avg_power_watts": 564.1989287815686,
      "data_parallel": 1,
      "energy_per_request_joules": 214.0293506470685,
      "energy_per_token_joules": 0.10234282857565034,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 64,
      "median_itl_ms": 11.219836000236683,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 5512.833059568223,
      "p90_itl_ms": 12.259755798731929,
      "p95_itl_ms": 13.49427009699866,
      "p99_itl_ms": 19.031764687388197,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 95.95238095238095,
      "avg_output_len": 2598.691919191919,
      "avg_power_watts": 587.9965155217465,
      "data_parallel": 1,
      "energy_per_request_joules": 227.7106430814092,
      "energy_per_token_joules": 0.08762510145959024,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 13.522330002160743,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 6710.366159095527,
      "p90_itl_ms": 14.995950201409869,
      "p95_itl_ms": 16.484720296284646,
      "p99_itl_ms": 21.25071637565269,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 95.98076923076923,
      "avg_output_len": 2285.659090909091,
      "avg_power_watts": 605.0788525244425,
      "data_parallel": 1,
      "energy_per_request_joules": 210.6784259201206,
      "energy_per_token_joules": 0.09217403713356308,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 96,
      "median_itl_ms": 14.309395104646683,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 6564.525883223105,
      "p90_itl_ms": 15.341871604323387,
      "p95_itl_ms": 16.327757388353348,
      "p99_itl_ms": 20.299079716205593,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 127.95505617977528,
      "avg_output_len": 2427.7929292929293,
      "avg_power_watts": 571.5198821995349,
      "data_parallel": 1,
      "energy_per_request_joules": 190.23467002210106,
      "energy_per_token_joules": 0.07835704096786583,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 128,
      "median_itl_ms": 17.091874964535236,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 7293.791025543127,
      "p90_itl_ms": 18.270541727542877,
      "p95_itl_ms": 19.189599715173234,
      "p99_itl_ms": 22.79142644256353,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 191.94444444444446,
      "avg_output_len": 2172.7146464646466,
      "avg_power_watts": 586.2491029119586,
      "data_parallel": 1,
      "energy_per_request_joules": 144.9583370332894,
      "energy_per_token_joules": 0.06671761396240401,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 192,
      "median_itl_ms": 20.807377994060516,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 8787.021418996119,
      "p90_itl_ms": 22.97214157879353,
      "p95_itl_ms": 24.080638960003853,
      "p99_itl_ms": 27.716139070689596,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 255.86111111111111,
      "avg_output_len": 2110.8030303030305,
      "avg_power_watts": 595.805656956448,
      "data_parallel": 1,
      "energy_per_request_joules": 125.37717899587997,
      "energy_per_token_joules": 0.05939785815916732,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 256,
      "median_itl_ms": 24.282067082822323,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 10030.759953664978,
      "p90_itl_ms": 26.272324286401272,
      "p95_itl_ms": 26.88558818772435,
      "p99_itl_ms": 30.299605559557666,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 383.8909090909091,
      "avg_output_len": 2227.698653198653,
      "avg_power_watts": 563.9737742939677,
      "data_parallel": 1,
      "energy_per_request_joules": 118.72531044477012,
      "energy_per_token_joules": 0.05329504970265962,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 384,
      "median_itl_ms": 34.59671100426931,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 10582.10429374688,
      "p90_itl_ms": 37.844318998395465,
      "p95_itl_ms": 39.02042949630413,
      "p99_itl_ms": 42.281922802794746,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 511.88235294117646,
      "avg_output_len": 2235.355218855219,
      "avg_power_watts": 579.2002271854865,
      "data_parallel": 1,
      "energy_per_request_joules": 108.58662061704894,
      "energy_per_token_joules": 0.04857689717550075,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 512,
      "median_itl_ms": 36.22605300915893,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 11923.36812071233,
      "p90_itl_ms": 43.71156719571445,
      "p95_itl_ms": 44.61976600141497,
      "p99_itl_ms": 47.697860816842876,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 767.7258064516129,
      "avg_output_len": 2174.170707070707,
      "avg_power_watts": 575.119645979908,
      "data_parallel": 1,
      "energy_per_request_joules": 99.5761978839005,
      "energy_per_token_joules": 0.045799622614758256,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 768,
      "median_itl_ms": 57.05505100195296,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 12557.300980785467,
      "p90_itl_ms": 64.27263838704675,
      "p95_itl_ms": 66.42261650122236,
      "p99_itl_ms": 75.26780993386625,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 1023.8571428571429,
      "avg_output_len": 2101.568181818182,
      "avg_power_watts": 590.7200429077185,
      "data_parallel": 1,
      "energy_per_request_joules": 93.86348932493158,
      "energy_per_token_joules": 0.044663547029782834,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 1024,
      "median_itl_ms": 65.82066099508666,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 13225.999325888979,
      "p90_itl_ms": 79.74796341150069,
      "p95_itl_ms": 82.23776889790315,
      "p99_itl_ms": 100.62445269781165,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 1535.567901234568,
      "avg_output_len": 2190.2345679012346,
      "avg_power_watts": 597.8349061797795,
      "data_parallel": 1,
      "energy_per_request_joules": 96.84387808550831,
      "energy_per_token_joules": 0.04421621295946752,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 1536,
      "median_itl_ms": 102.49504250532482,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 13520.717089177399,
      "p90_itl_ms": 119.3194648833014,
      "p95_itl_ms": 124.81068421184317,
      "p99_itl_ms": 155.6544672956806,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    },
    {
      "activated_params_billions": 4.0,
      "architecture": "MoE",
      "avg_batch_size": 2047.322033898305,
      "avg_output_len": 2379.013468013468,
      "avg_power_watts": 607.1288191082168,
      "data_parallel": 1,
      "energy_per_request_joules": 106.92206829182203,
      "energy_per_token_joules": 0.04494386842673256,
      "expert_parallel": 1,
      "gpu_model": "H100",
      "max_num_seqs": 2048,
      "median_itl_ms": 140.31523650919553,
      "model_id": "openai/gpt-oss-20b",
      "nickname": "GPT OSS 20B",
      "num_gpus": 1,
      "output_throughput_tokens_per_sec": 13508.601737252715,
      "p90_itl_ms": 162.23554299212992,
      "p95_itl_ms": 167.05090280593137,
      "p99_itl_ms": 223.2124011302948,
      "tensor_parallel": 1,
      "total_params_billions": 21.0,
      "weight_precision": "mxfp4"
    }
  ],
  "task": "gpqa",
  "task_display_name": "GPQA Diamond"
}
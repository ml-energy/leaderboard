{
  "activated_params_billions": 35.0,
  "architecture": "MoE",
  "configurations": [
    {
      "avg_batch_size": 7.453157529493407,
      "avg_output_len": 260.91796875,
      "avg_power_watts": 3526.352487859148,
      "energy_per_request_joules": 5114.360295035614,
      "energy_per_token_joules": 19.601410817113816,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 8,
      "median_itl_ms": 36.16602650436107,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          190,
          237,
          123,
          86,
          58,
          46,
          34,
          39,
          33,
          26,
          10,
          10,
          13,
          9,
          4,
          10,
          8,
          3,
          4,
          6,
          6,
          3,
          1,
          2,
          4,
          2,
          3,
          1,
          3,
          1,
          2,
          5,
          3,
          2,
          3,
          0,
          2,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          2,
          2,
          0,
          0,
          26
        ]
      },
      "output_throughput_tokens_per_sec": 179.90299375697597,
      "p90_itl_ms": 37.107014504726976,
      "p95_itl_ms": 41.37740798614686,
      "p99_itl_ms": 184.10287071019448,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 15.010050251256281,
      "avg_output_len": 247.185546875,
      "avg_power_watts": 3729.9100702400992,
      "energy_per_request_joules": 3031.859087689862,
      "energy_per_token_joules": 12.265519266881133,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 16,
      "median_itl_ms": 37.57659500115551,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          177,
          262,
          139,
          75,
          68,
          44,
          22,
          22,
          30,
          18,
          21,
          15,
          13,
          9,
          5,
          7,
          9,
          5,
          6,
          7,
          6,
          5,
          3,
          4,
          6,
          0,
          1,
          2,
          2,
          0,
          2,
          4,
          3,
          5,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          2,
          0,
          1,
          0,
          0,
          0,
          0,
          20
        ]
      },
      "output_throughput_tokens_per_sec": 304.09720037792886,
      "p90_itl_ms": 59.73097490787041,
      "p95_itl_ms": 139.1649049430269,
      "p99_itl_ms": 232.32184813125076,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 31.211382113821138,
      "avg_output_len": 249.193359375,
      "avg_power_watts": 3814.5532792182466,
      "energy_per_request_joules": 1962.8543296170474,
      "energy_per_token_joules": 7.876832410542832,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 32,
      "median_itl_ms": 40.554428996983916,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          187,
          244,
          129,
          88,
          59,
          41,
          31,
          38,
          26,
          17,
          16,
          18,
          14,
          7,
          7,
          4,
          10,
          5,
          6,
          6,
          4,
          6,
          3,
          5,
          3,
          2,
          4,
          2,
          2,
          2,
          3,
          4,
          3,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          22
        ]
      },
      "output_throughput_tokens_per_sec": 484.27503346556114,
      "p90_itl_ms": 144.01254728727508,
      "p95_itl_ms": 189.015151753847,
      "p99_itl_ms": 296.41245798760804,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 62.79761904761905,
      "avg_output_len": 264.3525390625,
      "avg_power_watts": 3847.1798745797246,
      "energy_per_request_joules": 1409.9191497586203,
      "energy_per_token_joules": 5.333480642019776,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 64,
      "median_itl_ms": 42.34188899863511,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          186,
          232,
          143,
          88,
          51,
          45,
          29,
          30,
          22,
          23,
          19,
          20,
          10,
          6,
          5,
          13,
          10,
          7,
          3,
          3,
          3,
          4,
          5,
          8,
          2,
          4,
          2,
          2,
          1,
          2,
          2,
          3,
          2,
          3,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          2,
          0,
          1,
          28
        ]
      },
      "output_throughput_tokens_per_sec": 721.3263031780325,
      "p90_itl_ms": 193.41042281012054,
      "p95_itl_ms": 249.2693369917106,
      "p99_itl_ms": 350.7250541169195,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 126.16438356164383,
      "avg_output_len": 249.1484375,
      "avg_power_watts": 3837.8308185638825,
      "energy_per_request_joules": 1074.278762003518,
      "energy_per_token_joules": 4.311802123998944,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 128,
      "median_itl_ms": 63.51270950108301,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          176,
          255,
          125,
          82,
          63,
          59,
          27,
          21,
          24,
          29,
          12,
          20,
          10,
          13,
          7,
          15,
          6,
          4,
          7,
          6,
          1,
          2,
          6,
          1,
          5,
          3,
          3,
          3,
          1,
          0,
          0,
          1,
          0,
          4,
          5,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          2,
          3,
          0,
          0,
          0,
          0,
          19
        ]
      },
      "output_throughput_tokens_per_sec": 890.0758217087475,
      "p90_itl_ms": 271.6060373117216,
      "p95_itl_ms": 331.02105044817995,
      "p99_itl_ms": 427.8845228199498,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 254.03496503496504,
      "avg_output_len": 251.0341796875,
      "avg_power_watts": 3809.936970092912,
      "energy_per_request_joules": 952.7380764813234,
      "energy_per_token_joules": 3.7952524141028916,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 256,
      "median_itl_ms": 100.34475001157261,
      "num_gpus": 4,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          179,
          244,
          133,
          89,
          53,
          49,
          40,
          32,
          24,
          11,
          18,
          14,
          14,
          14,
          12,
          8,
          8,
          5,
          8,
          4,
          5,
          5,
          3,
          2,
          2,
          2,
          3,
          0,
          1,
          5,
          0,
          0,
          1,
          2,
          2,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          2,
          0,
          22
        ]
      },
      "output_throughput_tokens_per_sec": 1003.8691908700073,
      "p90_itl_ms": 379.9528354022186,
      "p95_itl_ms": 450.08266218937933,
      "p99_itl_ms": 607.1932697686134,
      "parallelization": {
        "data_parallel": 4,
        "expert_parallel": 4,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 8.0,
      "avg_output_len": 257.7822265625,
      "avg_power_watts": 6448.003317212598,
      "energy_per_request_joules": 8850.598405235156,
      "energy_per_token_joules": 34.333625414199396,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 8,
      "median_itl_ms": 25.78650100622326,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          177,
          250,
          132,
          85,
          57,
          48,
          35,
          27,
          16,
          22,
          27,
          17,
          7,
          11,
          11,
          9,
          4,
          7,
          6,
          2,
          6,
          6,
          3,
          3,
          3,
          1,
          2,
          0,
          2,
          2,
          4,
          2,
          1,
          2,
          1,
          3,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          2,
          0,
          0,
          0,
          26
        ]
      },
      "output_throughput_tokens_per_sec": 187.80432416979448,
      "p90_itl_ms": 26.64559380209539,
      "p95_itl_ms": 38.15994059841599,
      "p99_itl_ms": 292.29877839941764,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 14.304136253041362,
      "avg_output_len": 244.6640625,
      "avg_power_watts": 6758.250279384601,
      "energy_per_request_joules": 5754.972414113171,
      "energy_per_token_joules": 23.52193597747185,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 16,
      "median_itl_ms": 28.52964100020472,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          190,
          239,
          128,
          82,
          63,
          39,
          40,
          31,
          24,
          24,
          21,
          18,
          14,
          12,
          5,
          8,
          13,
          3,
          8,
          5,
          3,
          2,
          3,
          1,
          0,
          2,
          3,
          2,
          2,
          0,
          1,
          1,
          1,
          3,
          6,
          2,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          2,
          0,
          1,
          0,
          0,
          1,
          18
        ]
      },
      "output_throughput_tokens_per_sec": 287.31692348186476,
      "p90_itl_ms": 85.16325570089974,
      "p95_itl_ms": 204.9073873982705,
      "p99_itl_ms": 391.7437532526672,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 29.612903225806452,
      "avg_output_len": 256.099609375,
      "avg_power_watts": 7129.253923070689,
      "energy_per_request_joules": 4357.457798877102,
      "energy_per_token_joules": 17.01469912238948,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 32,
      "median_itl_ms": 29.703827000048477,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          175,
          242,
          132,
          94,
          58,
          48,
          35,
          33,
          32,
          10,
          14,
          14,
          6,
          16,
          6,
          7,
          7,
          8,
          11,
          2,
          5,
          3,
          1,
          4,
          6,
          4,
          5,
          2,
          3,
          2,
          1,
          3,
          1,
          4,
          1,
          1,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          23
        ]
      },
      "output_throughput_tokens_per_sec": 419.00558286625073,
      "p90_itl_ms": 203.1592576007824,
      "p95_itl_ms": 296.5374268504092,
      "p99_itl_ms": 469.438188058412,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 60.58313817330211,
      "avg_output_len": 239.1767578125,
      "avg_power_watts": 7385.2441363868265,
      "energy_per_request_joules": 3398.3464163170615,
      "energy_per_token_joules": 14.208514436762949,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 64,
      "median_itl_ms": 32.022907995269634,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          172,
          246,
          133,
          94,
          67,
          36,
          33,
          37,
          24,
          16,
          22,
          10,
          11,
          12,
          10,
          10,
          10,
          11,
          3,
          7,
          8,
          5,
          3,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          3,
          0,
          2,
          2,
          2,
          2,
          0,
          0,
          1,
          1,
          1,
          0,
          2,
          0,
          2,
          1,
          0,
          0,
          0,
          12
        ]
      },
      "output_throughput_tokens_per_sec": 519.7759533029244,
      "p90_itl_ms": 308.59219859994505,
      "p95_itl_ms": 392.906518394011,
      "p99_itl_ms": 578.4235467162215,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 125.0984126984127,
      "avg_output_len": 240.8505859375,
      "avg_power_watts": 7522.557587214518,
      "energy_per_request_joules": 3037.5290448585984,
      "energy_per_token_joules": 12.611673885015286,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 128,
      "median_itl_ms": 107.98230000364129,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          171,
          244,
          148,
          90,
          61,
          49,
          31,
          29,
          21,
          25,
          9,
          15,
          8,
          18,
          10,
          8,
          6,
          5,
          5,
          5,
          3,
          2,
          4,
          6,
          4,
          5,
          3,
          1,
          4,
          2,
          6,
          0,
          0,
          3,
          0,
          0,
          1,
          0,
          2,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          14
        ]
      },
      "output_throughput_tokens_per_sec": 596.4757458684796,
      "p90_itl_ms": 436.2059984006919,
      "p95_itl_ms": 539.0646853018551,
      "p99_itl_ms": 766.7574844890623,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 253.89302325581394,
      "avg_output_len": 250.6787109375,
      "avg_power_watts": 7556.786625043494,
      "energy_per_request_joules": 2786.7057302326198,
      "energy_per_token_joules": 11.116642972236322,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 256,
      "median_itl_ms": 157.17038699949626,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          176,
          258,
          135,
          81,
          59,
          45,
          33,
          27,
          19,
          20,
          9,
          18,
          12,
          14,
          9,
          13,
          8,
          9,
          7,
          6,
          3,
          5,
          3,
          4,
          5,
          4,
          3,
          2,
          0,
          3,
          1,
          1,
          1,
          2,
          0,
          2,
          0,
          0,
          0,
          0,
          0,
          0,
          2,
          0,
          0,
          2,
          0,
          0,
          0,
          23
        ]
      },
      "output_throughput_tokens_per_sec": 679.772359687765,
      "p90_itl_ms": 587.3382220015628,
      "p95_itl_ms": 674.923033991945,
      "p99_itl_ms": 1007.5633156971888,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 507.980198019802,
      "avg_output_len": 245.1513671875,
      "avg_power_watts": 7563.230842384139,
      "energy_per_request_joules": 2327.099058260932,
      "energy_per_token_joules": 9.492498797614653,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 512,
      "median_itl_ms": 46.97110698907636,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          171,
          257,
          127,
          85,
          62,
          50,
          29,
          29,
          29,
          15,
          22,
          16,
          13,
          10,
          13,
          4,
          12,
          8,
          4,
          2,
          6,
          7,
          3,
          1,
          3,
          3,
          2,
          0,
          1,
          1,
          2,
          4,
          1,
          3,
          1,
          2,
          3,
          2,
          1,
          1,
          0,
          1,
          0,
          0,
          2,
          0,
          0,
          0,
          0,
          16
        ]
      },
      "output_throughput_tokens_per_sec": 796.7586832125472,
      "p90_itl_ms": 753.1906669901218,
      "p95_itl_ms": 958.4689879920916,
      "p99_itl_ms": 1457.4133462883765,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 761.196261682243,
      "avg_output_len": 248.56396484375,
      "avg_power_watts": 7582.7057491068,
      "energy_per_request_joules": 1882.0266629256457,
      "energy_per_token_joules": 7.571598981005586,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 768,
      "median_itl_ms": 135.23087599605788,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          360,
          494,
          268,
          166,
          127,
          82,
          70,
          56,
          60,
          53,
          31,
          22,
          22,
          24,
          11,
          16,
          11,
          19,
          10,
          10,
          6,
          7,
          8,
          3,
          15,
          3,
          4,
          6,
          3,
          6,
          5,
          2,
          3,
          6,
          2,
          3,
          0,
          3,
          0,
          0,
          2,
          2,
          3,
          1,
          0,
          1,
          1,
          2,
          0,
          39
        ]
      },
      "output_throughput_tokens_per_sec": 1001.4668986206318,
      "p90_itl_ms": 986.1245049978606,
      "p95_itl_ms": 1235.3415320030763,
      "p99_itl_ms": 1523.5468456987294,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 1016.08,
      "avg_output_len": 246.30794270833334,
      "avg_power_watts": 7573.722383794504,
      "energy_per_request_joules": 1641.981287517274,
      "energy_per_token_joules": 6.666375714329414,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 1024,
      "median_itl_ms": 412.0534945031977,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          548,
          741,
          419,
          251,
          175,
          138,
          85,
          99,
          67,
          57,
          52,
          53,
          35,
          40,
          27,
          18,
          17,
          15,
          16,
          12,
          14,
          17,
          12,
          7,
          7,
          8,
          4,
          9,
          8,
          8,
          4,
          4,
          8,
          7,
          7,
          1,
          0,
          4,
          3,
          1,
          1,
          2,
          3,
          0,
          1,
          4,
          3,
          0,
          2,
          58
        ]
      },
      "output_throughput_tokens_per_sec": 1136.107940558277,
      "p90_itl_ms": 1197.2823849937413,
      "p95_itl_ms": 1452.9929094969702,
      "p99_itl_ms": 2222.534037558218,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    },
    {
      "avg_batch_size": 2033.1137440758293,
      "avg_output_len": 244.664794921875,
      "avg_power_watts": 7567.486945317497,
      "energy_per_request_joules": 1319.4210380803609,
      "energy_per_token_joules": 5.392770294155606,
      "gpu_model": "B200",
      "max_num_batched_tokens": null,
      "max_num_seqs": 2048,
      "median_itl_ms": 90.50702999229543,
      "num_gpus": 8,
      "output_length_distribution": {
        "bins": [
          2.0,
          42.92,
          83.84,
          124.76,
          165.68,
          206.60000000000002,
          247.52,
          288.44,
          329.36,
          370.28000000000003,
          411.20000000000005,
          452.12,
          493.04,
          533.96,
          574.88,
          615.8000000000001,
          656.72,
          697.64,
          738.5600000000001,
          779.48,
          820.4000000000001,
          861.32,
          902.24,
          943.1600000000001,
          984.08,
          1025.0,
          1065.92,
          1106.8400000000001,
          1147.76,
          1188.68,
          1229.6000000000001,
          1270.52,
          1311.44,
          1352.3600000000001,
          1393.28,
          1434.2,
          1475.1200000000001,
          1516.04,
          1556.96,
          1597.88,
          1638.8000000000002,
          1679.72,
          1720.64,
          1761.5600000000002,
          1802.48,
          1843.4,
          1884.3200000000002,
          1925.24,
          1966.16,
          2007.0800000000002,
          2048.0
        ],
        "counts": [
          728,
          964,
          563,
          315,
          251,
          171,
          152,
          122,
          96,
          87,
          75,
          66,
          46,
          39,
          37,
          20,
          31,
          20,
          30,
          21,
          11,
          12,
          23,
          13,
          12,
          19,
          12,
          10,
          7,
          3,
          5,
          5,
          5,
          9,
          7,
          9,
          6,
          1,
          1,
          4,
          1,
          2,
          8,
          2,
          1,
          1,
          2,
          5,
          2,
          64
        ]
      },
      "output_throughput_tokens_per_sec": 1403.2652111139857,
      "p90_itl_ms": 1760.6246729992563,
      "p95_itl_ms": 2032.0430309948279,
      "p99_itl_ms": 2345.212553998863,
      "parallelization": {
        "data_parallel": 8,
        "expert_parallel": 8,
        "notes": "DP for attention, EP for MLP experts",
        "tensor_parallel": 1
      }
    }
  ],
  "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
  "output_length_distribution": {
    "bins": [
      2.0,
      42.92,
      83.84,
      124.76,
      165.68,
      206.60000000000002,
      247.52,
      288.44,
      329.36,
      370.28000000000003,
      411.20000000000005,
      452.12,
      493.04,
      533.96,
      574.88,
      615.8000000000001,
      656.72,
      697.64,
      738.5600000000001,
      779.48,
      820.4000000000001,
      861.32,
      902.24,
      943.1600000000001,
      984.08,
      1025.0,
      1065.92,
      1106.8400000000001,
      1147.76,
      1188.68,
      1229.6000000000001,
      1270.52,
      1311.44,
      1352.3600000000001,
      1393.28,
      1434.2,
      1475.1200000000001,
      1516.04,
      1556.96,
      1597.88,
      1638.8000000000002,
      1679.72,
      1720.64,
      1761.5600000000002,
      1802.48,
      1843.4,
      1884.3200000000002,
      1925.24,
      1966.16,
      2007.0800000000002,
      2048.0
    ],
    "counts": [
      3963,
      5409,
      2977,
      1851,
      1332,
      990,
      726,
      672,
      547,
      453,
      378,
      346,
      248,
      254,
      179,
      170,
      170,
      134,
      134,
      104,
      90,
      91,
      84,
      66,
      79,
      64,
      55,
      44,
      42,
      39,
      41,
      39,
      35,
      58,
      39,
      26,
      18,
      15,
      11,
      11,
      11,
      10,
      25,
      9,
      15,
      15,
      13,
      9,
      7,
      430
    ]
  },
  "task": "sourcegraph-fim",
  "total_params_billions": 480.0,
  "weight_precision": "fp8"
}